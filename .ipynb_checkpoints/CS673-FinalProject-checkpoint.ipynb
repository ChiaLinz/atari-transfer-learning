{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d62341",
   "metadata": {},
   "source": [
    "# CS673 Deep Learning | Final Project\n",
    "\n",
    "## Proposal -  Improved Bot Learning process on Atari games by using Transfer Learning\n",
    "\n",
    "## Team: \n",
    "- Ching-Hao Sun\n",
    "- Chia-Lin Hsieh\n",
    "- Rahul Gautham Putcha\n",
    "\n",
    "## Index\n",
    "- [Abstract](#Abstract)\n",
    "- [Setting up Google Drive for Colab (Recommended for Training)](#Setting-up-Google-Drive-for-Colab-(Recommended-for-Training))\n",
    "- [Baseline Models](#Baseline-Models:)\n",
    "- [Candidate ML Models / Methods](#Candidate-ML-Models-/-Methods)\n",
    "- [Project Environment Setup](#Project-Environment-Setup)\n",
    "  - [Part 1: Installation of GYM](#Part-1)\n",
    "  - [Part 2: Reinforcement Learning Dependencies](#Part-2)\n",
    "- [Working on the Project: PART I - Learning the first game](#Working-on-the-Project:-PART-I---Learning-the-first-game)\n",
    "  - [Hyperparameters](#Hyperparameters)\n",
    "  - [About Reinforcement Learning](#About-Reinforcement-Learning) : (Yet-to-update)\n",
    "  - [Replay Memory](#Replay-Memory)\n",
    "  - [Agent](#Agent)\n",
    "  - [Starting the Game Environment](#Starting-the-Game-Environment)\n",
    "  - [A short demo: Of how the model predicts](#A-short-demo:-Of-how-the-model-predicts)\n",
    "  - [Q-Learning](#Q-Learning)\n",
    "  - [Model Checkpointing](#Model-Checkpointing)\n",
    "- [Transfer Learning: PART II - Learning the second game](#Working-on-the-Project:-PART-I---Learning-the-first-game)\n",
    "\n",
    "\n",
    "## Abstract\n",
    "Reinforcement learning algorithms require tens of thousands or millions of time steps -\n",
    "which is equivalent to several weeks of training in real time to learn how to play a\n",
    "single game. Having a bot trained from scratch is costly in terms of time and processing\n",
    "power.\n",
    "\n",
    "Suppose we have a pre-trained model of a bot that has already learnt to play one game. \n",
    "We intend to make use of the same trained-model for a bot in learning another game of \n",
    "a similar traits/environment, thereby improving the efficiency of learning the second \n",
    "game and expanding the botâ€™s knowledge in tackling multiple games in less time.\n",
    "\n",
    "## Baseline Models:\n",
    "CNN (Convolutional Neural Network) with DQN (Deep-Q-Network; a Q-Learning variant)\n",
    "\n",
    "\n",
    "## Candidate ML Models / Methods\n",
    "- Deep Convolutional Neural Network\n",
    "- Deep Q-Network\n",
    "- Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba0e11",
   "metadata": {},
   "source": [
    "## Setting up Google Drive for Colab (Recommended for Training)\n",
    "Before carring out this section please visit the Git Repo for this repository and find IPYNB **[FIRST-PRIORITY-RUN]: Project Setup File** before this file.\n",
    "\n",
    "- [GitHub project repo](https://github.com/RPG-coder/atari-transfer-learning)\n",
    "\n",
    "Feel free to skip this process if you are doing locally on a GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6e62164",
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c387203",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"drive\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7768737",
   "metadata": {},
   "outputs": [],
   "source": [
    "cd \"MyDrive/CS677DeepLearning/atari-transfer-learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc4ae2",
   "metadata": {},
   "source": [
    "## Project Environment Setup\n",
    "\n",
    "### Part 1\n",
    "#### Requirements:\n",
    "  - Development Environment Window (Installation procedure is similar for Linux and Mac too...)\n",
    "  - Miniconda or Anaconda with conda cmd installed \n",
    "\n",
    "\n",
    "#### Install Microsoft Visual Studio 2022 (For Windows only)\n",
    "  - Select Build Tools Desktop Development with C++\n",
    "\n",
    "#### Installation process\n",
    "- Open a Terminal (For example: Command prompt) ... Require Conda cmd installed by using Miniconda installation setup\n",
    "- Setup a new environment **(Recommended)**\\\n",
    "    <code>$ conda create -n env3</code>\n",
    "    \n",
    "    <code>$ conda activate env3</code>\n",
    "\n",
    "- Install Necessary Package in our new environments\n",
    "\n",
    "  - Install Python3.7 \\\n",
    "    <code>$ conda install python=3.7</code>\n",
    "    \n",
    "  - Install OpenAI Gym for Atari games \\\n",
    "    <code>$ pip install gym[atari]</code>\n",
    "\n",
    "- With this project comes a git repository where you can download the project folder structure and the necessary file after environment setup shown above.\n",
    "   - The link to [GitHub project repo](https://github.com/RPG-coder/atari-transfer-learning)\n",
    "\n",
    "\n",
    "- After Setting up the repo locally into your computer, put all of your atari game into the './roms' folder.\n",
    "- Choose a Atari game from any of the following sources or your choice:\n",
    "  - [Breakout from oldgames.sk](https://www.oldgames.sk/en/game/breakout/download/8314/)\n",
    "  - [SpaceInvaders from consoleroms.com](https://www.consoleroms.com/roms/atari-2600/space-invaders)\n",
    "  - [SpaceInvaders from atarimania.com](http://www.atarimania.com/game-atari-2600-vcs-space-invaders_s6947.html)\n",
    "\n",
    "- Also, you can see by default the roms folder contains Breakout and SpaceInvaders '.bin' files in it.\n",
    "- After putting all of your games that you want to run in this project, go back to the terminal where you are running conda environment.\n",
    "- Run following cmd to load the game into the arcade learning environment (A way for us to use the atari games using open-ai gym) \\\n",
    "    <code>$ ale-import-roms /roms</code>\n",
    "\n",
    "**You are now all set to Run this project...**\n",
    "\n",
    "FacedError: If not all's set, no need to worry. Execute below project steps sequentially to get all dependencies setup in no time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a5065",
   "metadata": {},
   "source": [
    "### Making Gym[Atari] work on our localhost\n",
    "At first we load the games by importing the Arcade Learning Environment package. we uploaded the games using ale-import-roms into this program and use it inside gym emulator. \n",
    "\n",
    "This is a setup tutorial, if you have already done with the setup feel free to skip and proceed to [Part 2](#Part-2) or [Working on the Project](#Working-on-the-Project:-PART-I---Learning-the-first-game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbe99c",
   "metadata": {},
   "source": [
    "This project requires Python 3.7 and gym[atari]==0.19.0\n",
    "\n",
    "**Execute below line (START to END) if you are on Google Colab**\\"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8783ea8",
   "metadata": {},
   "source": [
    "!apt-get install python3.7 # GOOGLE COLAB only: Install the python versionr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b28603",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**else execute below line in conda CLI environment such as env3 as mentioned above. GOOGLE COLAB may not require bellow steps as it is already pre-configured with packages.**\n",
    "\n",
    "For Example,\\\n",
    "Conda Terminal or CMD prompt(Windows) or Terminal(Linux or Mac OS)\\\n",
    "<code>(env3) path> conda uninstall python</code>\\\n",
    "<code>(env3) path> conda install python=3.7</code>\n",
    "\n",
    "**START**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffc34964",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Only on Local machine\n",
    "!conda uninstall python\n",
    "!conda install python=3.7"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0c65ab6",
   "metadata": {},
   "source": [
    "!pip uninstall gym\n",
    "!pip install gym[atari]==0.21.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cce76696",
   "metadata": {},
   "source": [
    "!pip install ale_py\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df016e9c",
   "metadata": {},
   "source": [
    "**END**\n",
    "\n",
    "**(Mandatary execution)** Executing below step will import games that are necessary for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51900677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66567d64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[SUPPORTED]    \u001b[0m             breakout              roms\\Breakout.bin\n",
      "\u001b[92m[SUPPORTED]    \u001b[0m       space_invaders         roms\\SpaceInvaders.bin\n",
      "\n",
      "\n",
      "\n",
      "Imported 2 / 2 ROMs\n"
     ]
    }
   ],
   "source": [
    "!ale-import-roms roms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea1c7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\miniconda3\\envs\\cs677\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n",
      "C:\\Users\\rahul\\miniconda3\\envs\\cs677\\lib\\site-packages\\ale_py\\roms\\__init__.py:94: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  _RESOLVED_ROMS = _resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from ale_py import ALEInterface\n",
    "from ale_py.roms import Breakout\n",
    "from ale_py.roms import SpaceInvaders\n",
    "\n",
    "ale = ALEInterface()        # Ignore any Deprecation warnings cause by this line\n",
    "ale.loadROM(Breakout)       # This line will load your Breakout game into this project\n",
    "ale.loadROM(SpaceInvaders)  # This line will load your SpaceInvaders game into this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0dbe2",
   "metadata": {},
   "source": [
    "Let Try to see the Breakout atari game inside of gym,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41694f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('BreakoutDeterministic-v4')#('SpaceInvaders-v4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e1f5e",
   "metadata": {},
   "source": [
    "Actions moves the bot can make in this game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d936c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This game supports 4 action moves\n",
      "The moves are ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "print(f\"This game supports {env.action_space.n} action moves\")\n",
    "print(f\"The moves are {env.unwrapped.get_action_meanings()}\") # Note that the NOOP means no operation or no move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd4cfd",
   "metadata": {},
   "source": [
    "A basic game play can be executed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "58f96f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode is finished after the 155 timesteps\n",
      "Episode info: {'lives': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset() # Start the game from beginning\n",
    "t=0 # timestamp (epoch)\n",
    "while True: # Run the game till the game is over, for every timestep\n",
    "    env.render() # COMMENT THIS LINE ON COLAB // Print the game to the screen ...\n",
    "    action = env.action_space.sample() # Random action\n",
    "    observation, reward, done, info = env.step(action) # At each step try random action\n",
    "    if done: # if the game is over (End of the game: can be win, lose or draw in any game) => Stop the game\n",
    "        print(\"Episode is finished after the {} timesteps\".format(t+1))\n",
    "        print(\"Episode info: {}\".format(info)) # What the reason? for the game to stop\n",
    "        break\n",
    "    t=t+1\n",
    "\n",
    "env.close() # Close the window\n",
    "print() # Just for format: This one just prints nothing so we can avoid the print of previous line in jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058e6b9",
   "metadata": {},
   "source": [
    "You will see the game window pop up and close automatically. \n",
    "\n",
    "If you did Hurray!! We are now able to work with any game using Gym in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba82d35",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8601b",
   "metadata": {},
   "source": [
    "Install Reinforcement Learning process dependencies\n",
    "\n",
    "**Execute below line (START to END) in conda CLI environment such as env3 as mentioned above. GOOGLE COLAB may already have following packages pre-installed**\n",
    "\n",
    "**START**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c5ecbfc",
   "metadata": {},
   "source": [
    "!pip uninstall tensorflow\n",
    "!pip install --upgrade tensorflow==2.7.0\n",
    "!pip install --upgrade tensorflow-gpu==2.7.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef2cac8d",
   "metadata": {},
   "source": [
    "!pip uninstall keras\n",
    "!pip install keras==2.7.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1167ece",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "!pip install pyglet\n",
    "!pip install scikit-image\n",
    "# For PIL\n",
    "!pip install pillow\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97dbb62",
   "metadata": {},
   "source": [
    "**END**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647d03d",
   "metadata": {},
   "source": [
    "## Working on the Project: PART I - Learning the first game\n",
    "If you run above commands we will see below import modules to be successfully executed in our program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb48b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python Libraries\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Gym for loading Atari Environment compatible for Reinforcement Learning\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Basic Data Science Libraries (Useful for Reinforcement Learning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import keras\n",
    "\n",
    "# Image Processing Libraries\n",
    "import cv2\n",
    "import imageio\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c0d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Deep Learning: Building Neural Network\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Flatten, Dense, Multiply, Concatenate, LeakyReLU, Lambda, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18951f3",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Below are the hyperparameters that we are using to tune the learning process of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8b31edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters space\n",
    "HYPERPARAMS = {\n",
    "    # Google COLAB Setting\n",
    "    \"ON_COLAB\": False,\n",
    "    \"COLAB_COMPATIBLE_MODEL\": False,  # We have seen that Colab's GYM env considers all 18 actions of Atari game machine\n",
    "                                     # This is alleveated by installing pyglet, !pip install pyglet\n",
    "                                     # This parameter say our model to consider 18 actions by default.\n",
    "                                     # In our project old project model can be use by setting above condition to True\n",
    "    \n",
    "    # Reinforcement Learning Parameters\n",
    "    \"ENV_NAME\"        : 'BreakoutDeterministic-v4', # Name of environment to be used\n",
    "    # ('SpaceInvaders-v0') # ('Assault-ram-v0')\n",
    "    \"MEM_SIZE\"        : 10000, # Size of replay memory\n",
    "    \"GAMMA\"           : 0.99,   # Gamma (Discount rate) of Markov decision process\n",
    "    \n",
    "    # Exploration vs Exploitation (for Epsilon Decay Policy)\n",
    "    \"EPSILON\"         : 1,     # Start agent at exploration stage=1, or exploitation=0\n",
    "    \"EPSILON_MIN\"     : 0.01,\n",
    "    \"EPSILON_DECAY\"   : 0.9995,\n",
    "    \"TOTAL_FRAMES\"    : 5000000,\n",
    "    \"EPSILON_MAX\"     : 1,\n",
    "    \n",
    "    # Model Training Hyper Parameters\n",
    "    \"LEARNING_RATE\"   : 0.0001,\n",
    "    \"MOMENTUM\"        : 0.001,\n",
    "    \"STACK_SIZE\"      : 4,\n",
    "    \"HIDDEN_NEURONS\"  : 512,  # Number of Neurons in the Deep Neural Network\n",
    "    \"MINIBATCH_SIZE\"  : 32,\n",
    "    \"NUM_EPISODES\"    : 5000, # Number of episodes/gameplay for the agent training\n",
    "    \"RETRAIN\"         : 100,  # Number of times the agent background model trains on its Replay memory before proceeding\n",
    "    \n",
    "    # Model used in Replay Memory for Exploitation (Learning to win) on What has be Explored (or What has been found).\n",
    "    \"TGT_UPDATE_FREQ\" : 1500,\n",
    "    \"NUM_EXPLORE\"     : 1000,\n",
    "    \n",
    "    # For demo purpose\n",
    "    \"VIS_DIR\"               : \"GIFs\", # For GOOGLE COLAB Environment only\n",
    "    \"AUTOSAVE_CHECKPOINT\"   : 100,   # Auto save model after a number of episode = 100\n",
    "    \"SAVED_MODEL_NAME\"      : \"model_dqn_breakout.h5\", # Name of final model, second game \"model_dqn_spaceshoot.h5\"\n",
    "    \"TRANSFER_MODEL_NAME\"   : \"model_dqn_breakout.h5\",\n",
    "    \"TMP_MODEL\"             : 'tmp_model_1.h5',        # TODO: Remove it as its not used yet??\n",
    "    \n",
    "    # Extra Tuning\n",
    "    \"NOOPMAX\"         : 8, # Maximum number of No operation actions taken at the beginning of the game (For using every exploration)\n",
    "    \n",
    "    # Testing\n",
    "    \"NUM_EVAL\"        : 20, # Number of Evals (Test runs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3dbb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists to store loss and reward value per game!\n",
    "LOSS_HISTORY = []\n",
    "REWARD_HISTORY = []\n",
    "\n",
    "# Number of Frame viewed OR number of times env.step(action) called.\n",
    "FRAME_COUNT = 0\n",
    "\n",
    "model_swap =1\n",
    "if not os.path.exists(\"models\"): \n",
    "    os.mkdir(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973e18a",
   "metadata": {},
   "source": [
    "### About Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56db728",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "Replay Memory for improving the agent model by making it play (or fitting) over it past experience, stored in it's Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a7cec6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Boosting Experience\n",
    "class Replay_Memory:\n",
    "    '''\n",
    "        This replay memory clas would act as a buffer in which previous experiences would be stored. \n",
    "        Agent Experience = [ state=current_state, action=current_action, reward, next_state, done]\n",
    "    '''\n",
    "    def __init__(self, MEM_SIZE = 2000): \n",
    "        self.memory = deque(maxlen = MEM_SIZE)\n",
    "        self.max_size = MEM_SIZE\n",
    "    def add(self,  state, action, reward, next_state, done): \n",
    "        self.memory.append(( state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885004f1",
   "metadata": {},
   "source": [
    "### Agent\n",
    "The agent class: contains the following attribute,\n",
    "- Performs Learning using Reinforcement Learning\n",
    "- Saves/Loads the model\n",
    "- Contains Background model and Foreground model\n",
    "- Foreground model plays the game (Exploration)\n",
    "- Background model trains on its Explored Observation (or states/images/frames)\n",
    "- Background model is trained by means of using Replay Memory as mentioned above.\n",
    "- Foreground model is updated after every TGT_UPDATE_FREQ periods on swap count i.e.,(swap_count % TGT_UPDATE_FREQ == 0)\n",
    "- **Special:** Can also do Transfer Learning\n",
    "- Transfer Learning is a ability of making an agent that has learned to play one game to play another game of similar but higher complexity, in a short duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee6790fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model/Agent\n",
    "class Agent:\n",
    "    '''This class contains all methods for an agent to function.'''\n",
    "    \n",
    "    def __init__(self, env):\n",
    "        print(\"Setting up the agent ...\")\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = 18 if HYPERPARAMS[\"COLAB_COMPATIBLE_MODEL\"] else env.action_space.n\n",
    "        self.memory = Replay_Memory(HYPERPARAMS[\"MEM_SIZE\"])\n",
    "        self.gamma = HYPERPARAMS[\"GAMMA\"]\n",
    "        self.epsilon = HYPERPARAMS[\"EPSILON\"]\n",
    "        self.epsilon_max = HYPERPARAMS[\"EPSILON_MAX\"]\n",
    "        self.epsilon_min = HYPERPARAMS[\"EPSILON_MIN\"]\n",
    "        self.total_frame = HYPERPARAMS[\"TOTAL_FRAMES\"]\n",
    "        self.slope = (self.epsilon_max - self.epsilon_min)/self.total_frame\n",
    "        self.epsilon_decay = HYPERPARAMS[\"EPSILON_DECAY\"]\n",
    "        self.lr = HYPERPARAMS[\"LEARNING_RATE\"]\n",
    "        self.momentum = HYPERPARAMS[\"MOMENTUM\"]\n",
    "        self.dummy_input = np.zeros((1,self.action_size))\n",
    "        self.dummy_batch = np.zeros((HYPERPARAMS[\"MINIBATCH_SIZE\"],self.action_size))\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.get_tgt_model()\n",
    "        \n",
    "        print(\"Agent has been Sucessfully setup ...\")\n",
    "        \n",
    "    def reset_parameters(self, env):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        \n",
    "        self.action_size = 18 if HYPERPARAMS[\"COLAB_COMPATIBLE_MODEL\"] else env.action_space.n\n",
    "        self.memory = Replay_Memory(HYPERPARAMS[\"MEM_SIZE\"])\n",
    "        self.gamma = HYPERPARAMS[\"GAMMA\"]\n",
    "        self.epsilon = HYPERPARAMS[\"EPSILON\"]\n",
    "        self.epsilon_max = HYPERPARAMS[\"EPSILON_MAX\"]\n",
    "        self.epsilon_min = HYPERPARAMS[\"EPSILON_MIN\"]\n",
    "        self.total_frame = HYPERPARAMS[\"TOTAL_FRAMES\"]\n",
    "        self.slope = (self.epsilon_max - self.epsilon_min)/self.total_frame\n",
    "        self.epsilon_decay = HYPERPARAMS[\"EPSILON_DECAY\"]\n",
    "        self.lr = HYPERPARAMS[\"LEARNING_RATE\"]\n",
    "        self.momentum = HYPERPARAMS[\"MOMENTUM\"]\n",
    "        self.dummy_input = np.zeros((1,self.action_size))\n",
    "        self.dummy_batch = np.zeros((HYPERPARAMS[\"MINIBATCH_SIZE\"],self.action_size))\n",
    "        \n",
    "    # Function for Agent Model Setup\n",
    "    def lambda_out_shape(self, input_shape):\n",
    "        shape = list(input_shape)\n",
    "        shape[-1] = 1\n",
    "        return tuple(shape)\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        '''Model to train the agent'''\n",
    "        return self._build_compatible_model(self.action_size)\n",
    "\n",
    "    def transfer_learning(self, new_env, prev_model_pathname, old_actions_size, new_actions_size):\n",
    "        '''\n",
    "            Perform a transfer of knowledege about a game of similar less complex enviroment to this game.\n",
    "            Please supply:\n",
    "            - model_pathname: the location of model file that has already learnt to training on a game.\n",
    "            - old_action_size: the estimation of size for the already learnt game.\n",
    "            - new_action_size: the estimation of size for the current game being played.\n",
    "            Returns model\n",
    "        '''\n",
    "        self.model = self._build_compatible_model(old_actions_size, True, new_actions_size,prev_model_pathname)\n",
    "        self.target_model = self._build_compatible_model(old_actions_size, True, new_actions_size,prev_model_pathname)\n",
    "        self.reset_parameters(new_env)\n",
    "        return\n",
    "\n",
    "    def _build_compatible_model(self, actions_size, is_transfer_learning=False, new_actions_size=0, model_pathname=\"\"):\n",
    "        '''A single method to build model normally or build model after transfer learning from another game'''\n",
    "        prev_actions_size = actions_size\n",
    "\n",
    "        agent_bot = self\n",
    "        input_layer = Input(shape=(84, 84, HYPERPARAMS[\"STACK_SIZE\"]), name=\"image\")     # Sending the stack of 4 resized image of 84*84\n",
    "\n",
    "        # Convolution-Max Pooling parts\n",
    "        conv_layer1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', name=\"conv2D_1\")(input_layer)\n",
    "        # May Introduce Max Pooling here ...\n",
    "        conv_layer2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', name=\"conv2D_2\")(conv_layer1)\n",
    "        # May Introduce Max Pooling here ...\n",
    "        conv_layer3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', name=\"conv2D_3\")(conv_layer2)\n",
    "\n",
    "        # Densely connected Neural Network\n",
    "        flat_feature = Flatten(name=\"flat_1\")(conv_layer3)                               # Input Layer\n",
    "        hidden_feature = Dense(HYPERPARAMS[\"HIDDEN_NEURONS\"], name=\"hidden_layer_1\")(flat_feature)      # Hidden Layer\n",
    "        lrelu_feature = LeakyReLU(name=\"activation_layer\")(hidden_feature)               # using Leaky-Rely activation with alpha=0.3 on Hidden Layer\n",
    "\n",
    "        # Setting up the Output Layer\n",
    "        q_value_prediction = Dense(prev_actions_size, name=\"q_values\")(lrelu_feature)\n",
    "\n",
    "        # Get Single Action and Target Q value\n",
    "        action_one_hot = Input(shape=(prev_actions_size,), name=\"action\")          # Take Current Action to be played\n",
    "        select_q_value_of_action = Multiply()([q_value_prediction,action_one_hot]) # Checking the Q-value of current move/action\n",
    "        target_q_value = Lambda(lambda x:K.max(x, axis=-1, keepdims=True),output_shape=agent_bot.lambda_out_shape)(select_q_value_of_action)\n",
    "\n",
    "        model = Model(inputs=[input_layer,action_one_hot], outputs=[q_value_prediction, target_q_value])\n",
    "\n",
    "        if is_transfer_learning:\n",
    "            # Load model for previous game\n",
    "            model.load_weights(model_pathname)\n",
    "            \n",
    "            # Sibling layers for learning actions\n",
    "            prev_Qlayer = model.get_layer(name=\"q_values\")\n",
    "            prev_Qlayer._name = \"old_action_q_values\"\n",
    "            new_Qlayer = Dense(new_actions_size, name=\"new_action_q_values\")\n",
    "\n",
    "            # Merge the Sibling layers to form one layer to estimate Q-values\n",
    "            q_value_prediction = Concatenate(name=\"q_values\")([prev_Qlayer(lrelu_feature), new_Qlayer(lrelu_feature)])\n",
    "\n",
    "            # Get Single Action and Target Q value\n",
    "            action_one_hot = Input(shape=(prev_actions_size+new_actions_size,), name=\"action\")\n",
    "            select_q_value_of_action = Multiply()([\n",
    "                q_value_prediction,\n",
    "                action_one_hot\n",
    "            ])  \n",
    "            target_q_value = Lambda(lambda x:K.max(x, axis=-1, keepdims=True),output_shape=agent_bot.lambda_out_shape)(select_q_value_of_action)\n",
    "\n",
    "            model = Model(inputs=[input_layer,action_one_hot], outputs=[q_value_prediction, target_q_value])\n",
    "            #print(id(model))\n",
    "        \n",
    "        #print(id(model), is_transfer_learning)\n",
    "        model.compile(loss=['mse','mse'], loss_weights=[0.0,1.0],optimizer=Adam(agent_bot.lr))\n",
    "        return model\n",
    "        \n",
    "    def get_tgt_model(self):\n",
    "        '''This method would clone the architecture as well as the initial weights of the base model into target model'''\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "        return self.target_model\n",
    "    \n",
    "    def update_target_model(self): \n",
    "        '''This method would update weights of target model'''\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def load_model(self, pathname): \n",
    "        '''This method would load weights of model'''\n",
    "        # load weights into new model\n",
    "        self.model.load_weights(pathname)\n",
    "        self.target_model = self.get_tgt_model()\n",
    "        print(\"Loaded model from disk\")\n",
    "        \n",
    "    def save_model(self, pathname):\n",
    "        '''Save method would save weights of model model'''\n",
    "        # serialize weights to HDF5\n",
    "        self.model.save_weights(pathname)\n",
    "        self.target_model = self.get_tgt_model()\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "\n",
    "    # Agent Play Prediction function\n",
    "    def next_action(self, state):\n",
    "        '''Get the next action using epsilon greedy policy for deciding whether to exploit or explore'''\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min: # Epsilon \n",
    "            self.epsilon = self.epsilon_max - self.slope*(FRAME_COUNT)\n",
    "            \n",
    "            \n",
    "        if (np.random.rand() <= self.epsilon):\n",
    "            return env.action_space.sample()\n",
    "        print(self.dummy_input.shape)\n",
    "        q_values = self.model.predict([np.expand_dims(state,axis=0),self.dummy_input])[0]\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    # Replay Functions\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        '''Store the experience in our replay memory'''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def replay(self, batch_size, model_swap):\n",
    "        '''\n",
    "            Does the back propogation to adjust weights during exploitation action.\n",
    "            - batch_size: total number of random samples that the agent can recollect from memory\n",
    "            - The higher the batch_size more is the time for training process.\n",
    "        '''\n",
    "        # REINFORCEMENT LEARNING\n",
    "        print(\"Game Play Paused! Model is training on it's past Memory\")\n",
    "        # First we set all input to NOOP or no move for every observations(stack of frames or images)\n",
    "        # Dummy_Inputs_batch.shape = [(MINBATCH_SIZE = 32 images), (action_size = 4 moves for breakout)]\n",
    "        dummy_batch = np.zeros((batch_size,18 if HYPERPARAMS[\"COLAB_COMPATIBLE_MODEL\"] else self.action_size)) \n",
    "            \n",
    "            \n",
    "        # Experience batch set\n",
    "        state_batch      = []\n",
    "        action_batch     = []\n",
    "        reward_batch     = []\n",
    "        next_state_batch = []\n",
    "        terminal_batch   = []  # recording Is_done?\n",
    "        \n",
    "        # Actual Move that should have played (This is also an Assumption)\n",
    "        y_batch = []\n",
    "\n",
    "        # Sample random minibatch of transition from replay memory\n",
    "        minibatch = random.sample(list(self.memory.memory), batch_size)\n",
    "        # For every experience thats in our Replay Memory\n",
    "        for data in minibatch: # We organize the data\n",
    "            state_batch.append(data[0])\n",
    "            action_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            next_state_batch.append(data[3])\n",
    "            terminal_batch.append(data[4])\n",
    "        \n",
    "        # Convert the is_done to a numpy array\n",
    "        terminal_batch = np.array(terminal_batch)\n",
    "\n",
    "         \n",
    "        for i in np.arange(HYPERPARAMS[\"RETRAIN\"]):\n",
    "            # Get what agent is assuming with the trained model till now. Supplying NOOP/NoAction input for every move... \n",
    "            # Model is predicting the Q-Value or we can all it as Future reward from current move (or action) made\n",
    "            target_q_values_batch = self.target_model.predict([np.float32(np.array(next_state_batch)), self.dummy_batch])[0]\n",
    "            # What model should assume (The Assumption is to predict its own output without any gameover) Outrageous!!\n",
    "            y_batch = reward_batch + (1 - terminal_batch) * self.gamma * np.max(target_q_values_batch, axis=-1)\n",
    "            # (1 - terminal_batch) above is to indicate the game is_done(0, return only reward for bad move) or in_progress(1, return reward with discounted sum)\n",
    "            # y_batch is also called Future Reward or the reward model is expecting to get in the future.\n",
    "\n",
    "\n",
    "            a_one_hot = np.zeros((batch_size, self.action_size))\n",
    "            #print(batch_size,self.action_size)\n",
    "            for index,action in enumerate(action_batch):\n",
    "                a_one_hot[index,action] = 1.0            # Get the Action the player performed previously\n",
    "\n",
    "            # START TRAINING PROCESS\n",
    "            # Get the loss between NoAction and the Expected Action that model suplies\n",
    "            loss = self.model.train_on_batch([np.float32(np.array(state_batch)),a_one_hot], [self.dummy_batch,y_batch])\n",
    "\n",
    "            if i == HYPERPARAMS[\"RETRAIN\"]-1: # Append loss to it's history, only on the last re-train loop\n",
    "                LOSS_HISTORY.append(loss[1])\n",
    "        \n",
    "            # END TRAINING PROCESS\n",
    "        \n",
    "        #At target network's update frequency, update the target network\n",
    "        if(model_swap % HYPERPARAMS[\"TGT_UPDATE_FREQ\"] == 0):\n",
    "            self.update_target_model()  # Swap Model\n",
    "            print(\"Target model swapped successfully with Trained model!\")\n",
    "            \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7e8ee6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def preprocess(image):\n",
    "    '''This method downsamples and resizes the images to 84*84 and converts it to grayscale for CNN compatibility and processing efficiency'''\n",
    "    # Downsample(image) & resize image into square for CNN compatibility\n",
    "    resized_image = preprocess_rgb(image)\n",
    "    grayscale_image = rgb2gray(resized_image)\n",
    "    return grayscale_image\n",
    "\n",
    "def preprocess_rgb(image):\n",
    "    '''This method downsamples and resizes the images to 84*84 and converts it to grayscale for CNN compatibility and processing efficiency'''\n",
    "    # Downsample(image) & resize image into square for CNN compatibility\n",
    "    resized_image = cv2.resize(image[::2, ::2], (84, 84), interpolation = cv2.INTER_AREA)\n",
    "    return resized_image\n",
    "\n",
    "def generate_gif(frame_no, frames, reward, path, e):\n",
    "    '''Utility method to generate gif from frames'''\n",
    "    for idx, frame_idx in enumerate(frames): \n",
    "        frames[idx] = resize(frame_idx, (420, 320, 3), preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{path}{\"episode_{0}_frame_{1}_reward_{2}.gif\".format(e, frame_no, reward)}', frames, duration=1/100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9af08",
   "metadata": {},
   "source": [
    "### Starting the Game Environment\n",
    "As seen previously we have installed the environment for running any game within gym emulator setup. Now is the time to get things working in action, for the main aim of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1e955804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\miniconda3\\envs\\cs677\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n",
      "C:\\Users\\rahul\\miniconda3\\envs\\cs677\\lib\\site-packages\\ale_py\\roms\\__init__.py:94: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  _RESOLVED_ROMS = _resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "from ale_py.roms import Breakout, SpaceInvaders,Tetris\n",
    "ale = ALEInterface()\n",
    "ale.loadROM(Breakout) #ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b7c716e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(HYPERPARAMS[\"ENV_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b0dff4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENVIRONMENT BreakoutDeterministic-v4:\n",
      "This environment requires 4 actions.\n",
      "The actions are ['NOOP', 'FIRE', 'RIGHT', 'LEFT'].\n"
     ]
    }
   ],
   "source": [
    "print(f'ENVIRONMENT {HYPERPARAMS[\"ENV_NAME\"]}:')\n",
    "print(f'This environment requires {env.action_space.n} actions.')\n",
    "print(f'The actions are {env.unwrapped.get_action_meanings()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c62ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the agent ...\n",
      "Agent has been Sucessfully setup ...\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f5574",
   "metadata": {},
   "source": [
    "### A short demo: Of how the model predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259284a",
   "metadata": {},
   "source": [
    "For the first step we are delivering the **STACK_SIZE=4** number of images at a time in our agent model. Along with this is the current action being played. The action signifies the last move that was played, i.e. the move played by in the last image of the 4 image stack/sequence.\n",
    "\n",
    "You may wonder why we are considering the **STACK_SIZE**. Firstly, DeepMind choose to use the past 4 frames. Why?\n",
    "1. frame doesnot describe the movement of player or the enemies or any items. (Relative motion of any object)\n",
    "2. frames bare minimum requirement to learn about the speed of objects. (We capture the relative position on object between 2 frames)\n",
    "3. frames is necessary to infer acceleration. Why? \n",
    "   - Every frame we are received with, provides the derivative of position w.r.t time.\n",
    "4. and so on..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "92977583",
   "metadata": {},
   "source": [
    "# Also, Please Run this in the Local environment, where gym takes in playable action rather than complete 18 Action as in COLAB.\n",
    "# Then Rerun, agent=Agent(env)\n",
    "env.reset()\n",
    "image1, image2, image3, image4 = preprocess(env.step(1)[0]), preprocess(env.step(2)[0]), preprocess(env.step(0)[0]), preprocess(env.step(0)[0])\n",
    "cv2.imshow(\"image1\", image1)\n",
    "cv2.imshow(\"image2\", image2)\n",
    "cv2.imshow(\"image3\", image3)\n",
    "cv2.imshow(\"image4\", image4)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db0382",
   "metadata": {},
   "source": [
    "4 windows pop up showcasing how the images look like. \n",
    "\n",
    "**Warning: Hit Space or any button to Resume. Else, your IPython Kernel may die/crash.**\n",
    "\n",
    "By executing below you are letting make it's first prediction. As said before the model takes STACK_SIZE=4 images and the current action (in one hot encoded format)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "90e669bc",
   "metadata": {},
   "source": [
    "# Also, Please Run this in the Local environment, where gym takes in playable action rather than complete 18 Action as in COLAB.\n",
    "# Make Sure that HYPERPARAMS[\"COLAB_COMPATIBLE_MODEL\"]=False for this\n",
    "# Then Rerun, agent=Agent(env)\n",
    "env.reset()\n",
    "image1, image2, image3, image4 = preprocess(env.step(1)[0]), preprocess(env.step(2)[0]), preprocess(env.step(0)[0]), preprocess(env.step(0)[0])\n",
    "agent.model.predict([\n",
    "    np.expand_dims(\n",
    "        np.stack([\n",
    "            image1, image2, image3, image4\n",
    "        ], axis=2), \n",
    "        axis=0\n",
    "    ), np.array([[0,1,0,0]],dtype='float')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7eca92",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "The first four floats, shown above, are the Q-values for each action move that can be played in the game. Also, the Q-values are for the action for the current state. The maximum of these Q-values is the **target output** or y, which we use in our **REPLAY MEMORY OF AGENT** as the expected output.\n",
    "\n",
    "Process of Reinforcement Learning using Q-Learning\n",
    "- Get Q-values from Neural Network\n",
    "- use Target_Qvalue_action_i = r+max(Q-values), or \n",
    "  - Future Reward for the action_i on state_s is ( current_reward + max(next_predicted_future_reward) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb94bd",
   "metadata": {},
   "source": [
    "### Model Checkpointing\n",
    "During the training process we found that it is really inefficient to produce the complete training model of 30hrs in a single run. It's better to work checkpoint of 5-10 hrs and save the progress in middle. For this we have devised the load_model and save_model functionality within our agent class.\n",
    "\n",
    "Use below function to check the trained model performance after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6b667",
   "metadata": {},
   "source": [
    "**Convert below line to Code block by pressing 'Y', if you already have a saved model. Make sure that the model file exist.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c072f494",
   "metadata": {},
   "source": [
    "agent.load_model('./models/model_dqn_breakout.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c37470",
   "metadata": {},
   "source": [
    "**OR** run Training blocks to run initial exploration stage, for our agent to get an understanding of the game, and then the Training. Exploration is not exactly understanding, but is a way for us to fill the replay memory with images of the game being played with random actions.\n",
    "\n",
    "### Game Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e7c21e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE = 1\n",
    "TRAIN   = 2\n",
    "TEST    = 3\n",
    "\n",
    "def slow_start(env,image_stack=[], rgb_stack=[],NOOPMAX=10):\n",
    "    idle_times = random.randint(4, NOOPMAX)\n",
    "    \n",
    "    #print(f'Agent is Staying Idle for {idle_times} times. Agent is thinking about what move to make...')\n",
    "    for idle_time in range(idle_times):\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]==False: env.render() # NOTE: Comment this in Google Colab\n",
    "        state, reward, done, info = env.step(0) # Zero means: NOOP or No Operation\n",
    "        processed_frame = preprocess(state)\n",
    "        image_stack.append(processed_frame)\n",
    "        rgb_stack.append(preprocess_rgb(state))\n",
    "        \n",
    "    return image_stack, rgb_stack\n",
    "\n",
    "def gameplay(PLAY_TYPE=TEST,MAX_EPISODE_PLAYTIME=1000000):\n",
    "    global FRAME_COUNT\n",
    "    #print(f'Agent is starting a new game: {e} games played.')\n",
    "    \n",
    "    # Reset Game\n",
    "    state = env.reset()\n",
    "    times_rewarded = times_penalized = 0\n",
    "    last_lives = 5\n",
    "    terminal_life_lost = False # False if last_lives==0 else True\n",
    "    \n",
    "    #print('Agent has made the start move.')\n",
    "    # Start the game by 'FIRE' action, incase if it doesnot start the game without it\n",
    "    state, _, _, _ = env.step(1) \n",
    "    \n",
    "    # Fill agent's memory with random times of no operation played\n",
    "    image_stack,rgb_stack = slow_start(env=env, NOOPMAX=HYPERPARAMS[\"NOOPMAX\"])\n",
    "    \n",
    "    \n",
    "    #print(f'Agent is now playing the game...')\n",
    "    i, state = 0, np.stack(image_stack[-4:], axis = 2)\n",
    "    while i < MAX_EPISODE_PLAYTIME:\n",
    "        agent.epsilon = 1  # Agent is Exploring the game by default\n",
    "        \n",
    "        # If agent has lost a life then start the game with 'FIRE' again.\n",
    "        if(terminal_life_lost == True):\n",
    "            state, _, _, _ = env.step(1) # 'FIRE' to start the game\n",
    "            slow_start(env, image_stack, rgb_stack, HYPERPARAMS[\"NOOPMAX\"])\n",
    "            state = np.stack(image_stack[-4:], axis = 2)\n",
    "\n",
    "        FRAME_COUNT = FRAME_COUNT + 1\n",
    "        action = env.action_space.sample() if PLAY_TYPE==EXPLORE else (agent.next_action(state) if PLAY_TYPE==TRAIN else np.argmax(agent.model.predict([np.expand_dims(state,axis=0),agent.dummy_input])[0]))\n",
    "\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]==False: env.render() # NOTE: Comment this in Google Colab\n",
    "        # Agent Makes random moves here...\n",
    "        next_state, reward, done, info = env.step(action if action < env.action_space.n else env.action_space.sample())\n",
    "        \n",
    "        rgb_stack.append(preprocess_rgb(next_state))\n",
    "        \n",
    "        # Agent updates it's game status here...\n",
    "        terminal_life_lost = True if info['ale.lives'] < last_lives else done\n",
    "        last_lives = info['ale.lives']\n",
    "        \n",
    "            \n",
    "        if reward > 0:      times_rewarded = times_rewarded + 1\n",
    "        elif reward < 0: times_penalized = times_penalized + 1\n",
    "        elif terminal_life_lost: times_penalized = times_penalized + 1\n",
    "        # Making the starting experience of rewards more fruitful. For our replay memory...\n",
    "        reward = 10 if reward > 0 else (-30 if reward < 0 else reward)\n",
    "        reward = -30 if terminal_life_lost else reward\n",
    "        \n",
    "        # Store the stack of images for new a experience\n",
    "        processed_frame = preprocess(next_state)\n",
    "        image_stack = image_stack[-3:]\n",
    "        image_stack.append(processed_frame)\n",
    "        \n",
    "        next_state = np.stack(image_stack[-4:], axis = 2)\n",
    "        if(len(image_stack) != 4): print(\"Something's not right!! The stack size is less than expected.\")\n",
    "            \n",
    "        #Store experience in replay mem\n",
    "        if(PLAY_TYPE==EXPLORE or PLAY_TYPE==TRAIN): \n",
    "            agent.store_experience(state, action, reward, next_state, terminal_life_lost)\n",
    "        state = next_state\n",
    "        \n",
    "        if done: break\n",
    "        i+=1\n",
    "        \n",
    "    REWARD_HISTORY.append(times_rewarded)\n",
    "    return image_stack, times_rewarded, times_penalized, rgb_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1bef6",
   "metadata": {},
   "source": [
    "### TRAINING STAGE\n",
    "This step is recommended to be executed on Google Colab or over machine with GPU.\n",
    "\n",
    "**CONVERT THE BLOCKS to Code block. BELOW for TRAINING.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e84041",
   "metadata": {},
   "source": [
    "#### Initial Exploration stage\n",
    "This process takes around 3 min 28 seconds for Breakout"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2562795f",
   "metadata": {},
   "source": [
    "'''\n",
    "    THIS IS DONE TO POPULATE REPLAY MEMORY WITH COMPLETE EXPLORATION TO INITIALIZE THE MEMORY \n",
    "'''\n",
    "total_times_rewarded=total_times_penalized=0\n",
    "for e in range(HYPERPARAMS[\"NUM_EXPLORE\"]):\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=EXPLORE, MAX_EPISODE_PLAYTIME=1000)\n",
    "    total_times_rewarded  = total_times_rewarded + times_rewarded\n",
    "    total_times_penalized = total_times_penalized+ times_penalized\n",
    "    if(e % 100 == 0): \n",
    "        print(\"Finished exploring for {} episodes\".format(e))\n",
    "        print(\"Total Times Rewarded: {}, Total Times Penalized: {}\".format(total_times_rewarded, total_times_penalized))\n",
    "\n",
    "print(\"EXPLORATION STEP COMPLETED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b1e7c",
   "metadata": {},
   "source": [
    "#### Train using DQN\n",
    "Every 100 episodes takes around 11-13 min for Breakout."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d97944ed",
   "metadata": {},
   "source": [
    "'''\n",
    "TRAIN DQN ON THE GAME\n",
    "'''\n",
    "rew_list = []\n",
    "e=0\n",
    "for e in range(1, HYPERPARAMS[\"NUM_EPISODES\"]+1):\n",
    "    print(\"Agent is ready for gameplay...\")\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=TRAIN, MAX_EPISODE_PLAYTIME=1000)\n",
    "            \n",
    "    rew_list.append(times_rewarded)\n",
    "            \n",
    "    if(e % HYPERPARAMS[\"AUTOSAVE_CHECKPOINT\"] == 0):\n",
    "        print(\"Finished episode \", e , \"/\", HYPERPARAMS[\"NUM_EPISODES\"], \" Total reward = \", sum(rew_list)/len(rew_list), \" eps = \", agent.epsilon)\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]:\n",
    "            if not os.path.exists(HYPERPARAMS[\"VIS_DIR\"]):\n",
    "                os.mkdir(HYPERPARAMS[\"VIS_DIR\"])\n",
    "            generate_gif(len(rgb_stack), rgb_stack, sum(rew_list)/(len(rew_list)), HYPERPARAMS[\"VIS_DIR\"] + \"/\", e)\n",
    "        \n",
    "        rew_list = []\n",
    "        print(f\"Saving Model Checkpoint at episode {e}...\")\n",
    "        agent.save_model(\"models/tmp_model_\" + str(e) + \".h5\")\n",
    "\n",
    "    # BACKPROP INITIATED AT THE END OF EVERY EPISODE AND NOT AT TGT_FREQ\n",
    "    agent.replay(HYPERPARAMS[\"MINIBATCH_SIZE\"], model_swap)\n",
    "    model_swap = model_swap + 1\n",
    "    \n",
    "print(f\"Agent is now prepared now, after training for {e} episodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e1414",
   "metadata": {},
   "source": [
    "#### Saving the final model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15851332",
   "metadata": {},
   "source": [
    "agent.save_model(agent.model, \"models/\" + HYPERPARAMS[\"MODEL_NAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512222e",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff2c1e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the agent ...\n",
      "Agent has been Sucessfully setup ...\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('BreakoutDeterministic-v4')\n",
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "904d424c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "agent.load_model(\"models/model_dqn_breakout.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa84b109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0 / 20  Total reward =  50\n",
      "Finished episode  1 / 20  Total reward =  50\n"
     ]
    }
   ],
   "source": [
    "rew_list = []\n",
    "total_times_rewarded=total_times_penalized=0\n",
    "for e in range(HYPERPARAMS[\"NUM_EVAL\"]):\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=TEST, MAX_EPISODE_PLAYTIME=1000)\n",
    "    \n",
    "    total_times_rewarded  = total_times_rewarded + times_rewarded\n",
    "    total_times_penalized = total_times_penalized+ times_penalized\n",
    "    rew_list.append(times_rewarded)\n",
    "    \n",
    "    print(\"Finished episode \", e , \"/\", HYPERPARAMS[\"NUM_EVAL\"], \" Total reward = \", times_rewarded*(10))\n",
    "    if HYPERPARAMS[\"ON_COLAB\"]:\n",
    "        if not os.path.exists(\"test\"):\n",
    "            os.mkdir(\"test\")\n",
    "        generate_gif(len(rgb_stack), rgb_stack, total_times_rewarded, \"test/\", e)\n",
    "\n",
    "print(\"Testing Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc8b9e",
   "metadata": {},
   "source": [
    "## Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2853d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the agent ...\n",
      "Agent has been Sucessfully setup ...\n"
     ]
    }
   ],
   "source": [
    "prev_env = gym.make('BreakoutDeterministic-v4')\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "agent = Agent(prev_env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9b1a44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning condition: Requires (prev_env.action_space.n <= env.action_space.n)\n",
    "# We are assuming the next game to be played is higher complex than old game we have trained.\n",
    "agent.transfer_learning(\n",
    "    env,\n",
    "    './models/model_dqn_breakout.h5', \n",
    "    prev_env.action_space.n, # Number of Old actions=4\n",
    "    (env.action_space.n-prev_env.action_space.n) # Number of New actions, Excluding old one's = 2\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2459ab",
   "metadata": {},
   "source": [
    "### Training (On new Game using model of old game)"
   ]
  },
  {
   "cell_type": "raw",
   "id": "998ae6fa",
   "metadata": {},
   "source": [
    "'''\n",
    "TRAIN DQN ON THE GAME\n",
    "'''\n",
    "rew_list = []\n",
    "e=0\n",
    "for e in range(1, HYPERPARAMS[\"NUM_EPISODES\"]+1):\n",
    "    print(\"Agent is ready for gameplay...\")\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=TRAIN, MAX_EPISODE_PLAYTIME=1000)\n",
    "            \n",
    "    rew_list.append(times_rewarded)\n",
    "            \n",
    "    if(e % HYPERPARAMS[\"AUTOSAVE_CHECKPOINT\"] == 0):\n",
    "        print(\"Finished episode \", e , \"/\", HYPERPARAMS[\"NUM_EPISODES\"], \" Total reward = \", sum(rew_list)/len(rew_list), \" eps = \", agent.epsilon)\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]:\n",
    "            if not os.path.exists(HYPERPARAMS[\"VIS_DIR\"]):\n",
    "                os.mkdir(HYPERPARAMS[\"VIS_DIR\"])\n",
    "            generate_gif(len(rgb_stack), rgb_stack, sum(rew_list)/(len(rew_list)), HYPERPARAMS[\"VIS_DIR\"] + \"/\", e)\n",
    "        \n",
    "        rew_list = []\n",
    "        print(f\"Saving Model Checkpoint at episode {e}...\")\n",
    "        agent.save_model(\"models/tmp_model_\" + str(e) + \".h5\")\n",
    "\n",
    "    # BACKPROP INITIATED AT THE END OF EVERY EPISODE AND NOT AT TGT_FREQ\n",
    "    agent.replay(HYPERPARAMS[\"MINIBATCH_SIZE\"], model_swap)\n",
    "    model_swap = model_swap + 1\n",
    "    \n",
    "print(f\"Agent is now prepared now, after training for {e} episodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3a180",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df5a30cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\miniconda3\\envs\\cs677\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0 / 20  Total reward =  210\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17484/589837055.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtotal_times_rewarded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_times_penalized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"NUM_EVAL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mimage_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes_rewarded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes_penalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgb_stack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgameplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_EPISODE_PLAYTIME\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtotal_times_rewarded\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtotal_times_rewarded\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimes_rewarded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_17484/1496985319.py\u001b[0m in \u001b[0;36mgameplay\u001b[1;34m(PLAY_TYPE, MAX_EPISODE_PLAYTIME)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mFRAME_COUNT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFRAME_COUNT\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mEXPLORE\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mTRAIN\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ON_COLAB\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# NOTE: Comment this in Google Colab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1766\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1401\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1403\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1163\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1165\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2002\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[0;32m   2003\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[1;32m-> 2004\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2005\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2006\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5457\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5458\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5459\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   5460\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5461\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   4531\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4533\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4534\u001b[0m     \u001b[1;31m# There is no graph to add in eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4535\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3243\u001b[0m     \"\"\"\n\u001b[0;32m   3244\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[1;32m-> 3245\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   3246\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3247\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3210\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3556\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3557\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3558\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3400\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3401\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3402\u001b[1;33m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[0;32m   3403\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3404\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\framework\\func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[1;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[0;32m   1182\u001b[0m         if x is not None)\n\u001b[0;32m   1183\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1184\u001b[1;33m     \u001b[0mfunc_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1185\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1186\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[0;32m    416\u001b[0m       \u001b[1;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m       \u001b[1;31m# and last_write_to_resource.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 418\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0minp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[1;32min\u001b[0m \u001b[0m_get_resource_inputs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    419\u001b[0m         \u001b[0mis_read\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mresource_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mResourceType\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mREAD_ONLY\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    420\u001b[0m         \u001b[0minput_id\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtensor_id\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\framework\\auto_control_deps.py\u001b[0m in \u001b[0;36m_get_resource_inputs\u001b[1;34m(op)\u001b[0m\n\u001b[0;32m    608\u001b[0m       \u001b[1;31m# TODO(srbs): An alternate would be to just compare the old and new set\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    609\u001b[0m       \u001b[1;31m# but that may not be as fast.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 610\u001b[1;33m       \u001b[0mupdated\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_acd_resource_resolvers_registry\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlookup\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreads\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwrites\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    611\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mupdated\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    612\u001b[0m         \u001b[1;31m# Conservatively remove any resources from `reads` that are also writes.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m_resource_resolver\u001b[1;34m(op, resource_reads, resource_writes)\u001b[0m\n\u001b[0;32m   6624\u001b[0m         \u001b[0mresource_writes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6625\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 6626\u001b[1;33m   if op.type in [\n\u001b[0m\u001b[0;32m   6627\u001b[0m       \u001b[1;34m\"IteratorGetNext\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"IteratorGetNextSync\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"IteratorGetNextAsOptional\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   6628\u001b[0m   ]:\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\framework\\ops.py\u001b[0m in \u001b[0;36mtype\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2507\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2508\u001b[0m     \u001b[1;34m\"\"\"The type of the op (e.g. `\"MatMul\"`).\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2509\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mpywrap_tf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_OperationOpType\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_c_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2510\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2511\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rew_list = []\n",
    "total_times_rewarded=total_times_penalized=0\n",
    "for e in range(HYPERPARAMS[\"NUM_EVAL\"]):\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=TEST, MAX_EPISODE_PLAYTIME=1000)\n",
    "    \n",
    "    total_times_rewarded  = total_times_rewarded + times_rewarded\n",
    "    total_times_penalized = total_times_penalized+ times_penalized\n",
    "    rew_list.append(times_rewarded)\n",
    "    \n",
    "    print(\"Finished episode \", e , \"/\", HYPERPARAMS[\"NUM_EVAL\"], \" Total reward = \", times_rewarded*(10))\n",
    "    if HYPERPARAMS[\"ON_COLAB\"]:\n",
    "        if not os.path.exists(\"test\"):\n",
    "            os.mkdir(\"test\")\n",
    "        generate_gif(len(rgb_stack), rgb_stack, total_times_rewarded, \"test/\", e)\n",
    "\n",
    "print(\"Testing Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53abaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "exit(1) # Close Gym Windows"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
