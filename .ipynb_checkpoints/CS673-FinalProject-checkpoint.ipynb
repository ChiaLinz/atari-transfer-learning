{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d62341",
   "metadata": {},
   "source": [
    "# CS673 Deep Learning | Final Project\n",
    "\n",
    "## Proposal -  Improved Bot Learning process on Atari games by using Transfer Learning\n",
    "\n",
    "## Team: \n",
    "- Ching-Hao Sun\n",
    "- Chia-Lin Hsieh\n",
    "- Rahul Gautham Putcha\n",
    "\n",
    "## Index\n",
    "- [Abstract](#Abstract)\n",
    "- [Setting up Google Drive for Colab (Recommended for Training)](#Setting-up-Google-Drive-for-Colab-(Recommended-for-Training))\n",
    "- [Baseline Models](#Baseline-Models:)\n",
    "- [Candidate ML Models / Methods](#Candidate-ML-Models-/-Methods)\n",
    "- [Project Environment Setup](#Project-Environment-Setup)\n",
    "  - [Part 1: Installation of GYM](#Part-1)\n",
    "  - [Part 2: Reinforcement Learning Dependencies](#Part-2)\n",
    "- [Working on the Project: PART I - Learning the first game](#Working-on-the-Project:-PART-I---Learning-the-first-game)\n",
    "  - [Hyperparameters](#Hyperparameters)\n",
    "  - [About Reinforcement Learning](#About-Reinforcement-Learning) : (Yet-to-update)\n",
    "  - [Replay Memory](#Replay-Memory)\n",
    "  - [Agent](#Agent)\n",
    "  - [Starting the Game Environment](#Starting-the-Game-Environment)\n",
    "  - [A short demo: Of how the model predicts](#A-short-demo:-Of-how-the-model-predicts)\n",
    "  - [Q-Learning](#Q-Learning)\n",
    "  - [Model Checkpointing](#Model-Checkpointing)\n",
    "- [Transfer Learning: PART II - Learning the second game](#Transfer-Learning:-PART-II)\n",
    "\n",
    "\n",
    "## Abstract\n",
    "Reinforcement learning algorithms require tens of thousands or millions of time steps -\n",
    "which is equivalent to several weeks of training in real time to learn how to play a\n",
    "single game. Having a bot trained from scratch is costly in terms of time and processing\n",
    "power.\n",
    "\n",
    "Suppose we have a pre-trained model of a bot that has already learnt to play one game. \n",
    "We intend to make use of the same trained-model for a bot in learning another game of \n",
    "a similar traits/environment, thereby improving the efficiency of learning the second \n",
    "game and expanding the botâ€™s knowledge in tackling multiple games in less time.\n",
    "\n",
    "## Baseline Models:\n",
    "CNN (Convolutional Neural Network) with DQN (Deep-Q-Network; a Q-Learning variant)\n",
    "\n",
    "\n",
    "## Candidate ML Models / Methods\n",
    "- Deep Convolutional Neural Network\n",
    "- Deep Q-Network\n",
    "- Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ba0e11",
   "metadata": {},
   "source": [
    "## Setting up Google Drive for Colab (Recommended for Training)\n",
    "Before carring out this section please visit the Git Repo for this repository and find IPYNB **[FIRST-PRIORITY-RUN]: Project Setup File** before this file.\n",
    "\n",
    "- [GitHub project repo](https://github.com/RPG-coder/atari-transfer-learning)\n",
    "\n",
    "Feel free to skip this process if you are doing locally on a GPU"
   ]
  },
  {
   "cell_type": "raw",
   "id": "422abb19",
   "metadata": {},
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a960e88",
   "metadata": {},
   "source": [
    "cd \"drive\""
   ]
  },
  {
   "cell_type": "raw",
   "id": "b442c57c",
   "metadata": {},
   "source": [
    "cd \"MyDrive/CS677DeepLearning/atari-transfer-learning\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc4ae2",
   "metadata": {},
   "source": [
    "## Project Environment Setup\n",
    "\n",
    "### Part 1\n",
    "#### Requirements:\n",
    "  - Development Environment Window (Installation procedure is similar for Linux and Mac too...)\n",
    "  - Miniconda or Anaconda with conda cmd installed \n",
    "\n",
    "\n",
    "#### Install Microsoft Visual Studio 2022 (For Windows only)\n",
    "  - Select Build Tools Desktop Development with C++\n",
    "\n",
    "#### Installation process\n",
    "- Open a Terminal (For example: Command prompt) ... Require Conda cmd installed by using Miniconda installation setup\n",
    "- Setup a new environment **(Recommended)**\\\n",
    "    <code>$ conda create -n env3</code>\n",
    "    \n",
    "    <code>$ conda activate env3</code>\n",
    "\n",
    "- Install Necessary Package in our new environments\n",
    "\n",
    "  - Install Python3.7 \\\n",
    "    <code>$ conda install python=3.7</code>\n",
    "    \n",
    "  - Install OpenAI Gym for Atari games \\\n",
    "    <code>$ pip install gym[atari]</code>\n",
    "\n",
    "- With this project comes a git repository where you can download the project folder structure and the necessary file after environment setup shown above.\n",
    "   - The link to [GitHub project repo](https://github.com/RPG-coder/atari-transfer-learning)\n",
    "\n",
    "\n",
    "- After Setting up the repo locally into your computer, put all of your atari game into the './roms' folder.\n",
    "- Choose a Atari game from any of the following sources or your choice:\n",
    "  - [Breakout from oldgames.sk](https://www.oldgames.sk/en/game/breakout/download/8314/)\n",
    "  - [SpaceInvaders from consoleroms.com](https://www.consoleroms.com/roms/atari-2600/space-invaders)\n",
    "  - [SpaceInvaders from atarimania.com](http://www.atarimania.com/game-atari-2600-vcs-space-invaders_s6947.html)\n",
    "\n",
    "- Also, you can see by default the roms folder contains Breakout and SpaceInvaders '.bin' files in it.\n",
    "- After putting all of your games that you want to run in this project, go back to the terminal where you are running conda environment.\n",
    "- Run following cmd to load the game into the arcade learning environment (A way for us to use the atari games using open-ai gym) \\\n",
    "    <code>$ ale-import-roms /roms</code>\n",
    "\n",
    "**You are now all set to Run this project...**\n",
    "\n",
    "FacedError: If not all's set, no need to worry. Execute below project steps sequentially to get all dependencies setup in no time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a5065",
   "metadata": {},
   "source": [
    "### Making Gym[Atari] work on our localhost\n",
    "At first we load the games by importing the Arcade Learning Environment package. we uploaded the games using ale-import-roms into this program and use it inside gym emulator. \n",
    "\n",
    "This is a setup tutorial, if you have already done with the setup feel free to skip and proceed to [Part 2](#Part-2) or [Working on the Project](#Working-on-the-Project:-PART-I---Learning-the-first-game)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5bbe99c",
   "metadata": {},
   "source": [
    "This project requires Python 3.7 and gym[atari]==0.19.0\n",
    "\n",
    "**Execute below line (START to END) if you are on Google Colab**\\"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c8783ea8",
   "metadata": {},
   "source": [
    "!apt-get install python3.7 # GOOGLE COLAB only: Install the python versionr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b28603",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**else execute below line in conda CLI environment such as env3 as mentioned above. GOOGLE COLAB may not require bellow steps as it is already pre-configured with packages.**\n",
    "\n",
    "For Example,\\\n",
    "Conda Terminal or CMD prompt(Windows) or Terminal(Linux or Mac OS)\\\n",
    "<code>(env3) path> conda uninstall python</code>\\\n",
    "<code>(env3) path> conda install python=3.7</code>\n",
    "\n",
    "**START**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ffc34964",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "# Only on Local machine\n",
    "!conda uninstall python\n",
    "!conda install python=3.7"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c0c65ab6",
   "metadata": {},
   "source": [
    "!pip uninstall gym\n",
    "!pip install gym[atari]==0.21.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "cce76696",
   "metadata": {},
   "source": [
    "!pip install ale_py\n",
    "!pip install pyglet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df016e9c",
   "metadata": {},
   "source": [
    "**END**\n",
    "\n",
    "**(Mandatary execution)** Executing below step will import games that are necessary for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51900677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ale_py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66567d64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[92m[SUPPORTED]    \u001b[0m             breakout              roms\\Breakout.bin\n",
      "\u001b[92m[SUPPORTED]    \u001b[0m       space_invaders         roms\\SpaceInvaders.bin\n",
      "\n",
      "\n",
      "\n",
      "Imported 2 / 2 ROMs\n"
     ]
    }
   ],
   "source": [
    "!ale-import-roms roms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ea1c7af",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\miniconda3\\envs\\cs677\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n",
      "C:\\Users\\rahul\\miniconda3\\envs\\cs677\\lib\\site-packages\\ale_py\\roms\\__init__.py:94: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  _RESOLVED_ROMS = _resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "from ale_py import ALEInterface\n",
    "from ale_py.roms import Breakout\n",
    "from ale_py.roms import SpaceInvaders\n",
    "\n",
    "ale = ALEInterface()        # Ignore any Deprecation warnings cause by this line\n",
    "ale.loadROM(Breakout)       # This line will load your Breakout game into this project\n",
    "ale.loadROM(SpaceInvaders)  # This line will load your SpaceInvaders game into this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0dbe2",
   "metadata": {},
   "source": [
    "Let Try to see the Breakout atari game inside of gym,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "41694f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('BreakoutDeterministic-v4')#('SpaceInvaders-v4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e1f5e",
   "metadata": {},
   "source": [
    "Actions moves the bot can make in this game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9d936c5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This game supports 4 action moves\n",
      "The moves are ['NOOP', 'FIRE', 'RIGHT', 'LEFT']\n"
     ]
    }
   ],
   "source": [
    "print(f\"This game supports {env.action_space.n} action moves\")\n",
    "print(f\"The moves are {env.unwrapped.get_action_meanings()}\") # Note that the NOOP means no operation or no move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd4cfd",
   "metadata": {},
   "source": [
    "A basic game play can be executed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "58f96f96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\miniconda3\\envs\\cs677\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode is finished after the 126 timesteps\n",
      "Episode info: {'ale.lives': 0}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "env.reset() # Start the game from beginning\n",
    "t=0 # timestamp (epoch)\n",
    "while True: # Run the game till the game is over, for every timestep\n",
    "    env.render() # COMMENT THIS LINE ON COLAB // Print the game to the screen ...\n",
    "    action = env.action_space.sample() # Random action\n",
    "    observation, reward, done, info = env.step(action) # At each step try random action\n",
    "    if done: # if the game is over (End of the game: can be win, lose or draw in any game) => Stop the game\n",
    "        print(\"Episode is finished after the {} timesteps\".format(t+1))\n",
    "        print(\"Episode info: {}\".format(info)) # What the reason? for the game to stop\n",
    "        break\n",
    "    t=t+1\n",
    "\n",
    "env.close() # Close the window\n",
    "print() # Just for format: This one just prints nothing so we can avoid the print of previous line in jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058e6b9",
   "metadata": {},
   "source": [
    "You will see the game window pop up and close automatically. \n",
    "\n",
    "If you did Hurray!! We are now able to work with any game using Gym in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba82d35",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8601b",
   "metadata": {},
   "source": [
    "Install Reinforcement Learning process dependencies\n",
    "\n",
    "**Execute below line (START to END) in conda CLI environment such as env3 as mentioned above. GOOGLE COLAB may already have following packages pre-installed**\n",
    "\n",
    "**START**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3c5ecbfc",
   "metadata": {},
   "source": [
    "!pip uninstall tensorflow\n",
    "!pip install --upgrade tensorflow==2.7.0\n",
    "!pip install --upgrade tensorflow-gpu==2.7.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ef2cac8d",
   "metadata": {},
   "source": [
    "!pip uninstall keras\n",
    "!pip install keras==2.7.0"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c1167ece",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "!pip install pyglet\n",
    "!pip install scikit-image\n",
    "# For PIL\n",
    "!pip install pillow\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97dbb62",
   "metadata": {},
   "source": [
    "**END**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647d03d",
   "metadata": {},
   "source": [
    "## Working on the Project: PART I - Learning the first game\n",
    "If you run above commands we will see below import modules to be successfully executed in our program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cb48b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python Libraries\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Gym for loading Atari Environment compatible for Reinforcement Learning\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Basic Data Science Libraries (Useful for Reinforcement Learning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import keras\n",
    "\n",
    "# Image Processing Libraries\n",
    "import cv2\n",
    "import imageio\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "21c0d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Deep Learning: Building Neural Network\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Flatten, Dense, Multiply, Concatenate, LeakyReLU, Lambda, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18951f3",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Below are the hyperparameters that we are using to tune the learning process of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8b31edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters space\n",
    "HYPERPARAMS = {\n",
    "    # Google COLAB Setting\n",
    "    \"ON_COLAB\": False,\n",
    "    \"COLAB_COMPATIBLE_MODEL\": False,  # We have seen that Colab's GYM env considers all 18 actions of Atari game machine\n",
    "                                      # This is alleveated by installing newer gym=0.21.0 and pyglet, !pip install pyglet\n",
    "                                      # This parameter say our model to consider 18 actions by default.\n",
    "                                      # In our project we mistakenly trained old project model on 18 action which made the training time high due to larger actions\n",
    "                                      # In case we are required to make use of backward compatible model we set this to TRUE\n",
    "    \n",
    "    # Reinforcement Learning Parameters\n",
    "    \"ENV_NAME\"        : 'BreakoutDeterministic-v4', # Name of environment to be used\n",
    "    # ('SpaceInvaders-v0') # ('Assault-ram-v0')\n",
    "    \"MEM_SIZE\"        : 20000, # Size of replay memory\n",
    "    \"GAMMA\"           : 0.99,   # Gamma (Discount rate) of Markov decision process\n",
    "    \n",
    "    # Exploration vs Exploitation (for Epsilon Decay Policy)\n",
    "    \"EPSILON\"         : 0.5,     # Start agent at exploration stage=1, or exploitation=0\n",
    "    \"EPSILON_MIN\"     : 0.01,\n",
    "    \"EPSILON_DECAY\"   : 0.9995,\n",
    "    \"TOTAL_FRAMES\"    : 5000000,\n",
    "    \"EPSILON_MAX\"     : 1,\n",
    "    \n",
    "    # Model Training Hyper Parameters\n",
    "    \"LEARNING_RATE\"   : 0.0001,\n",
    "    \"MOMENTUM\"        : 0.001,\n",
    "    \"STACK_SIZE\"      : 4,\n",
    "    \"HIDDEN_NEURONS\"  : 1024,  # Number of Neurons in the Deep Neural Network\n",
    "    \"MINIBATCH_SIZE\"  : 32,\n",
    "    \"NUM_EPISODES\"    : 1000, # Number of episodes/gameplay for the agent training\n",
    "    \"RETRAIN\"         : 10,  # Number of times the agent background model trains on its Replay memory before proceeding\n",
    "    \n",
    "    # Model used in Replay Memory for Exploitation (Learning to win) on What has be Explored (or What has been found).\n",
    "    \"TGT_UPDATE_FREQ\" : 1,\n",
    "    \"NUM_EXPLORE\"     : 1000,\n",
    "    \n",
    "    # For demo purpose\n",
    "    \"VIS_DIR\"               : \"GIFs\", # For GOOGLE COLAB Environment only\n",
    "    \"AUTOSAVE_CHECKPOINT\"   : 100,   # Auto save model after a number of episode = 100\n",
    "    \"SAVED_MODEL_NAME\"      : \"model_dqn_breakout.h5\", # Name of final model, second game \"model_dqn_spaceshoot.h5\"\n",
    "    \"TRANSFER_MODEL_NAME\"   : \"model_dqn_breakout.h5\",\n",
    "    \n",
    "    # Extra Tuning\n",
    "    \"NOOPMAX\"         : 8, # Maximum number of No operation actions taken at the beginning of the game (For using every exploration)\n",
    "    \n",
    "    # Testing\n",
    "    \"NUM_EVAL\"        : 20, # Number of Evals (Test runs)\n",
    "    \n",
    "    \"LIVES\"           : \"ale.lives\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d3dbb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists to store loss and reward value per game!\n",
    "LOSS_HISTORY = []\n",
    "REWARD_HISTORY = []\n",
    "\n",
    "# Number of Frame viewed OR number of times env.step(action) called.\n",
    "FRAME_COUNT = 0\n",
    "\n",
    "model_swap =1\n",
    "if not os.path.exists(\"models\"): \n",
    "    os.mkdir(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973e18a",
   "metadata": {},
   "source": [
    "### About Reinforcement Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56db728",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "Replay Memory for improving the agent model by making it play (or fitting) over it past experience, stored in it's Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a7cec6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Boosting Experience\n",
    "class Replay_Memory:\n",
    "    '''\n",
    "        This replay memory clas would act as a buffer in which previous experiences would be stored. \n",
    "        Agent Experience = [ state=current_state, action=current_action, reward, next_state, done]\n",
    "    '''\n",
    "    def __init__(self, MEM_SIZE = 2000): \n",
    "        self.memory = deque(maxlen = MEM_SIZE)\n",
    "        self.max_size = MEM_SIZE\n",
    "    def add(self,  state, action, reward, next_state, done): \n",
    "        self.memory.append(( state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885004f1",
   "metadata": {},
   "source": [
    "### Agent\n",
    "The agent class: contains the following attribute,\n",
    "- Performs Learning using Reinforcement Learning\n",
    "- Saves/Loads the model\n",
    "- Contains Background model and Foreground model\n",
    "- Foreground model plays the game (Exploration)\n",
    "- Background model trains on its Explored Observation (or states/images/frames)\n",
    "- Background model is trained by means of using Replay Memory as mentioned above.\n",
    "- Foreground model is updated after every TGT_UPDATE_FREQ periods on swap count i.e.,(swap_count % TGT_UPDATE_FREQ == 0)\n",
    "- **Special:** Can also do Transfer Learning\n",
    "- Transfer Learning is a ability of making an agent that has learned to play one game to play another game of similar but higher complexity, in a short duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ee6790fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model/Agent\n",
    "class Agent:\n",
    "    '''This class contains all methods for an agent to function.'''\n",
    "    \n",
    "    def __init__(self, env, action_size=None, model_pathname=\"\"):\n",
    "        print(\"Setting up the agent ...\")\n",
    "        self.reset_parameters(env, action_size)\n",
    "        self.model = self.build_model(model_pathname) if model_pathname else self.build_model()\n",
    "        self.target_model = self.get_tgt_model()\n",
    "        \n",
    "        print(\"Agent has been Sucessfully setup ...\")\n",
    "        \n",
    "    def reset_parameters(self, env, action_size=None):\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = 18 if HYPERPARAMS[\"COLAB_COMPATIBLE_MODEL\"] else env.action_space.n\n",
    "        if action_size: self.action_size = action_size\n",
    "        self.memory = Replay_Memory(HYPERPARAMS[\"MEM_SIZE\"])\n",
    "        self.gamma = HYPERPARAMS[\"GAMMA\"]\n",
    "        self.epsilon = HYPERPARAMS[\"EPSILON\"]\n",
    "        self.epsilon_max = HYPERPARAMS[\"EPSILON_MAX\"]\n",
    "        self.epsilon_min = HYPERPARAMS[\"EPSILON_MIN\"]\n",
    "        self.total_frame = HYPERPARAMS[\"TOTAL_FRAMES\"]\n",
    "        self.slope = (self.epsilon_max - self.epsilon_min)/self.total_frame\n",
    "        self.epsilon_decay = HYPERPARAMS[\"EPSILON_DECAY\"]\n",
    "        self.lr = HYPERPARAMS[\"LEARNING_RATE\"]\n",
    "        self.momentum = HYPERPARAMS[\"MOMENTUM\"]\n",
    "        self.dummy_input = np.zeros((1,self.action_size))\n",
    "        self.dummy_batch = np.zeros((HYPERPARAMS[\"MINIBATCH_SIZE\"],self.action_size))\n",
    "        \n",
    "    # Function for Agent Model Setup\n",
    "    def lambda_out_shape(self, input_shape):\n",
    "        shape = list(input_shape)\n",
    "        shape[-1] = 1\n",
    "        return tuple(shape)\n",
    "        \n",
    "    \n",
    "    def build_model(self, model_pathname=\"\"):\n",
    "        '''Model to train the agent'''\n",
    "        return self._build_compatible_model(self.action_size, model_pathname=model_pathname)\n",
    "\n",
    "    def transfer_learning(self, new_env, prev_model_pathname, old_actions_size, new_actions_size):\n",
    "        '''\n",
    "            Perform a transfer of knowledege about a game of similar less complex enviroment to this game.\n",
    "            Please supply:\n",
    "            - model_pathname: the location of model file that has already learnt to training on a game.\n",
    "            - old_action_size: the estimation of size for the already learnt game.\n",
    "            - new_action_size: the estimation of size for the current game being played.\n",
    "            Returns model\n",
    "        '''\n",
    "        self.model = self._build_compatible_model(old_actions_size, True, new_actions_size,prev_model_pathname)\n",
    "        self.target_model = self._build_compatible_model(old_actions_size, True, new_actions_size,prev_model_pathname)\n",
    "        self.reset_parameters(new_env)\n",
    "        return\n",
    "\n",
    "    def _build_compatible_model(self, actions_size, is_transfer_learning=False, new_actions_size=0, model_pathname=\"\"):\n",
    "        '''A single method to build model normally or build model after transfer learning from another game'''\n",
    "        prev_actions_size = actions_size\n",
    "        agent_bot = self\n",
    "        \n",
    "        input_layer = Input(shape=(84, 84, HYPERPARAMS[\"STACK_SIZE\"]), name=\"image\")     # Sending the stack of 4 resized image of 84*84\n",
    "\n",
    "        # Convolution-Max Pooling parts\n",
    "        conv_layer1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', use_bias=False, name=\"conv2D_1\", kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2))(input_layer)\n",
    "        # May Introduce Max Pooling here ...\n",
    "        conv_layer2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', use_bias=False, name=\"conv2D_2\", kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2))(conv_layer1)\n",
    "        # May Introduce Max Pooling here ...\n",
    "        conv_layer3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', use_bias=False, name=\"conv2D_3\", kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2))(conv_layer2)\n",
    "        # May Introduce Max Pooling here ...\n",
    "        conv_layer4 = Conv2D(HYPERPARAMS[\"HIDDEN_NEURONS\"], (7, 7), strides=(1, 1), activation='relu', use_bias=False, name=\"conv2D_4\", kernel_initializer=tf.keras.initializers.VarianceScaling(scale=2))(conv_layer3)\n",
    "\n",
    "        # Densely connected Neural Network\n",
    "        flat_feature = Flatten(name=\"flat_1\")(conv_layer4)\n",
    "        q_value_prediction = Dense(prev_actions_size, name=\"q_values\", activation='relu')(flat_feature) # Setting up the Output Layer\n",
    "        model = Model(inputs=[input_layer], outputs=[q_value_prediction])\n",
    "        \n",
    "        # LOAD AUTO_SAVE CHECKPOINT: If filename is provided and not for Transfer learning\n",
    "        if len(model_pathname)>0 and is_transfer_learning==False:\n",
    "            model.load_weights(model_pathname)\n",
    "\n",
    "        # PREFORM TRANSFER LEARNING FROM OLD MODEL: If filename is provided and not for Transfer learning\n",
    "        if is_transfer_learning and len(model_pathname)>0:\n",
    "            # Load model for previous game\n",
    "            model.load_weights(model_pathname)\n",
    "            \n",
    "            # Sibling layers for learning actions\n",
    "            prev_Qlayer = model.get_layer(name=\"q_values\")\n",
    "            prev_Qlayer._name = \"old_action_q_values\"\n",
    "            new_Qlayer = Dense(new_actions_size, name=\"new_action_q_values\", activation='relu')\n",
    "\n",
    "            # Merge the Sibling layers to form one layer to estimate Q-values\n",
    "            q_value_prediction = Concatenate(name=\"q_values\")([prev_Qlayer(flat_feature), new_Qlayer(flat_feature)])\n",
    "            model = Model(inputs=[input_layer], outputs=[q_value_prediction])\n",
    "        \n",
    "        model.compile(loss=['mse','mse'], loss_weights=[0.0,1.0],optimizer=Adam(agent_bot.lr))\n",
    "        return model\n",
    "        \n",
    "    def get_tgt_model(self):\n",
    "        '''This method would clone the architecture as well as the initial weights of the base model into target model'''\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "        return self.target_model\n",
    "    \n",
    "    def update_target_model(self): \n",
    "        '''This method would update weights of target model'''\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def load_model(self, pathname): \n",
    "        '''This method would load weights of model'''\n",
    "        # load weights into new model\n",
    "        self.model.load_weights(pathname)\n",
    "        self.target_model = self.get_tgt_model()\n",
    "        print(\"Loaded model from disk\")\n",
    "        \n",
    "    def save_model(self, pathname):\n",
    "        '''Save method would save weights of model model'''\n",
    "        # serialize weights to HDF5\n",
    "        self.model.save_weights(pathname)\n",
    "        self.target_model = self.get_tgt_model()\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "\n",
    "    # Agent Play Prediction function\n",
    "    def next_action(self, state):\n",
    "        '''Get the next action using epsilon greedy policy for deciding whether to exploit or explore'''\n",
    "        \n",
    "        if self.epsilon > self.epsilon_min: # Epsilon \n",
    "            self.epsilon = self.epsilon_max - self.slope*(FRAME_COUNT)\n",
    "            \n",
    "            \n",
    "        if (np.random.rand() <= self.epsilon):\n",
    "            return env.action_space.sample()\n",
    "        #print(self.dummy_input.shape)\n",
    "        q_values = self.model.predict([np.expand_dims(state,axis=0)])[0]\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    # Replay Functions\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        '''Store the experience in our replay memory'''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def replay(self, batch_size, model_swap):\n",
    "        '''\n",
    "            Does the back propogation to adjust weights during exploitation action.\n",
    "            - batch_size: total number of random samples that the agent can recollect from memory\n",
    "            - The higher the batch_size more is the time for training process.\n",
    "        '''\n",
    "        # REINFORCEMENT LEARNING\n",
    "        print(\"Game Play Paused! Model is training on it's past Memory\")\n",
    "        # First we set all input to NOOP or no move for every observations(stack of frames or images)\n",
    "        # Dummy_Inputs_batch.shape = [(MINBATCH_SIZE = 32 images), (action_size = 4 moves for breakout)]\n",
    "        dummy_batch = np.zeros((batch_size,18 if HYPERPARAMS[\"COLAB_COMPATIBLE_MODEL\"] else self.action_size)) \n",
    "            \n",
    "            \n",
    "        # Experience batch set\n",
    "        state_batch      = []\n",
    "        action_batch     = []\n",
    "        reward_batch     = []\n",
    "        next_state_batch = []\n",
    "        terminal_batch   = []  # recording Is_done?\n",
    "        \n",
    "        # Actual Move that should have played (This is also an Assumption)\n",
    "        y_batch = []\n",
    "\n",
    "        # Sample random minibatch of transition from replay memory\n",
    "        minibatch = random.sample(list(self.memory.memory), batch_size)\n",
    "        # For every experience thats in our Replay Memory\n",
    "        for data in minibatch: # We organize the data\n",
    "            state_batch.append(data[0])\n",
    "            action_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            next_state_batch.append(data[3])\n",
    "            terminal_batch.append(data[4])\n",
    "        \n",
    "        # Convert the is_done to a numpy array\n",
    "        terminal_batch = np.array(terminal_batch)\n",
    "\n",
    "         \n",
    "        for i in np.arange(HYPERPARAMS[\"RETRAIN\"]):\n",
    "            # Get what agent is assuming with the trained model till now. Supplying NOOP/NoAction input for every move... \n",
    "            # Model is predicting the Q-Value or we can all it as Future reward from current move (or action) made\n",
    "            target_q_values_batch = self.target_model.predict([np.float32(np.array(next_state_batch))])[0]\n",
    "            # What model should assume (The Assumption is to predict its own output without any gameover) Outrageous!!\n",
    "            y_batch = reward_batch + (1 - terminal_batch) * self.gamma * np.max(target_q_values_batch, axis=-1)\n",
    "            # (1 - terminal_batch) above is to indicate the game is_done(0, return only reward for bad move) or in_progress(1, return reward with discounted sum)\n",
    "            # y_batch is also called Future Reward or the reward model is expecting to get in the future.\n",
    "\n",
    "\n",
    "            #a_one_hot = np.zeros((batch_size, self.action_size))\n",
    "            #print(batch_size,self.action_size)\n",
    "            #for index,action in enumerate(action_batch):\n",
    "            #    a_one_hot[index,action] = 1.0            # Get the Action the player performed previously\n",
    "\n",
    "            # START TRAINING PROCESS\n",
    "            # Get the loss between NoAction and the Expected Action that model suplies\n",
    "            loss = self.model.train_on_batch([np.float32(np.array(state_batch))], [y_batch])\n",
    "\n",
    "            if i == HYPERPARAMS[\"RETRAIN\"]-1: # Append loss to it's history, only on the last re-train loop\n",
    "                LOSS_HISTORY.append(loss)\n",
    "        \n",
    "            # END TRAINING PROCESS\n",
    "        \n",
    "            #At target network's update frequency, update the target network\n",
    "        if(model_swap % HYPERPARAMS[\"TGT_UPDATE_FREQ\"] == 0):\n",
    "            self.update_target_model()  # Swap Model\n",
    "            print(\"Target model swapped successfully with Trained model!\")   \n",
    "            \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7e8ee6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def preprocess(image):\n",
    "    '''This method downsamples and resizes the images to 84*84 and converts it to grayscale for CNN compatibility and processing efficiency'''\n",
    "    # Downsample(image) & resize image into square for CNN compatibility\n",
    "    #resized_image = preprocess_rgb(image)\n",
    "    #grayscale_image = rgb2gray(resized_image)\n",
    "    \n",
    "    # crop image (top and bottom, top from 34, bottom remove last 16)\n",
    "    img = image[34:-16, :, :]\n",
    "    # resize image\n",
    "    img = cv2.resize(img, (84, 84))\n",
    "    img = img.mean(-1,keepdims=True)\n",
    "    img = img.astype('float32') / 255.\n",
    "    return img #grayscale_image\n",
    "\n",
    "def preprocess_rgb(image):\n",
    "    '''This method downsamples and resizes the images to 84*84 and converts it to grayscale for CNN compatibility and processing efficiency'''\n",
    "    # Downsample(image) & resize image into square for CNN compatibility\n",
    "    resized_image = cv2.resize(image[::2, ::2], (84, 84), interpolation = cv2.INTER_AREA)\n",
    "    return resized_image\n",
    "\n",
    "def generate_gif(frame_no, frames, reward, path, e):\n",
    "    '''Utility method to generate gif from frames'''\n",
    "    for idx, frame_idx in enumerate(frames): \n",
    "        frames[idx] = resize(frame_idx, (420, 320, 3), preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{path}{\"episode_{0}_frame_{1}_reward_{2}.gif\".format(e, frame_no, reward)}', frames, duration=1/100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9af08",
   "metadata": {},
   "source": [
    "### Starting the Game Environment\n",
    "As seen previously we have installed the environment for running any game within gym emulator setup. Now is the time to get things working in action, for the main aim of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e955804",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ale_py import ALEInterface\n",
    "from ale_py.roms import Breakout, SpaceInvaders,Tetris\n",
    "ale = ALEInterface()\n",
    "ale.loadROM(Breakout) #ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b7c716e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(HYPERPARAMS[\"ENV_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0dff4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENVIRONMENT BreakoutDeterministic-v4:\n",
      "This environment requires 4 actions.\n",
      "The actions are ['NOOP', 'FIRE', 'RIGHT', 'LEFT'].\n"
     ]
    }
   ],
   "source": [
    "print(f'ENVIRONMENT {HYPERPARAMS[\"ENV_NAME\"]}:')\n",
    "print(f'This environment requires {env.action_space.n} actions.')\n",
    "print(f'The actions are {env.unwrapped.get_action_meanings()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9c62ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the agent ...\n",
      "Agent has been Sucessfully setup ...\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f5574",
   "metadata": {},
   "source": [
    "### A short demo: Of how the model predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259284a",
   "metadata": {},
   "source": [
    "For the first step we are delivering the **STACK_SIZE=4** number of images at a time in our agent model. Along with this is the current action being played. The action signifies the last move that was played, i.e. the move played by in the last image of the 4 image stack/sequence.\n",
    "\n",
    "You may wonder why we are considering the **STACK_SIZE**. Firstly, DeepMind choose to use the past 4 frames. Why?\n",
    "1. frame doesnot describe the movement of player or the enemies or any items. (Relative motion of any object)\n",
    "2. frames bare minimum requirement to learn about the speed of objects. (We capture the relative position on object between 2 frames)\n",
    "3. frames is necessary to infer acceleration. Why? \n",
    "   - Every frame we are received with, provides the derivative of position w.r.t time.\n",
    "4. and so on..."
   ]
  },
  {
   "cell_type": "raw",
   "id": "a36d9d52",
   "metadata": {},
   "source": [
    "# Also, Please Run this in the Local environment, where gym takes in playable action rather than complete 18 Action as in COLAB.\n",
    "# Then Rerun, agent=Agent(env)\n",
    "env.reset()\n",
    "image1, image2, image3, image4 = preprocess(env.step(1)[0]), preprocess(env.step(2)[0]), preprocess(env.step(0)[0]), preprocess(env.step(0)[0])\n",
    "cv2.imshow(\"image1\", image1)\n",
    "cv2.imshow(\"image2\", image2)\n",
    "cv2.imshow(\"image3\", image3)\n",
    "cv2.imshow(\"image4\", image4)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db0382",
   "metadata": {},
   "source": [
    "4 windows pop up showcasing how the images look like. \n",
    "\n",
    "**Warning: Hit Space or any button to Resume. Else, your IPython Kernel may die/crash.**\n",
    "\n",
    "By executing below you are letting make it's first prediction. As said before the model takes STACK_SIZE=4 images and the current action (in one hot encoded format)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "ca538aa4",
   "metadata": {},
   "source": [
    "# Also, Please Run this in the Local environment, where gym takes in playable action rather than complete 18 Action as in COLAB.\n",
    "# Make Sure that HYPERPARAMS[\"COLAB_COMPATIBLE_MODEL\"]=False for this\n",
    "# Then Rerun, agent=Agent(env)\n",
    "env.reset()\n",
    "image1, image2, image3, image4 = preprocess(env.step(1)[0]), preprocess(env.step(2)[0]), preprocess(env.step(0)[0]), preprocess(env.step(0)[0])\n",
    "agent.model.predict([\n",
    "    np.expand_dims(\n",
    "        np.stack([\n",
    "            image1, image2, image3, image4\n",
    "        ], axis=2), \n",
    "        axis=0\n",
    "    )\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7eca92",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "The first four floats, shown above, are the Q-values for each action move that can be played in the game. Also, the Q-values are for the action for the current state. The maximum of these Q-values is the **target output** or y, which we use in our **REPLAY MEMORY OF AGENT** as the expected output.\n",
    "\n",
    "Process of Reinforcement Learning using Q-Learning\n",
    "- Get Q-values from Neural Network\n",
    "- use Target_Qvalue_action_i = r+max(Q-values), or \n",
    "  - Future Reward for the action_i on state_s is ( current_reward + max(next_predicted_future_reward) )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb94bd",
   "metadata": {},
   "source": [
    "### Model Checkpointing\n",
    "During the training process we found that it is really inefficient to produce the complete training model of 30hrs in a single run. It's better to work checkpoint of 5-10 hrs and save the progress in middle. For this we have devised the load_model and save_model functionality within our agent class.\n",
    "\n",
    "Use below function to check the trained model performance after training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ab6b667",
   "metadata": {},
   "source": [
    "**Convert below line to Code block by pressing 'Y', if you already have a saved model. Make sure that the model file exist.**"
   ]
  },
  {
   "cell_type": "raw",
   "id": "c072f494",
   "metadata": {},
   "source": [
    "agent.load_model('./models/model_dqn_breakout.h5') # \"models/\" + HYPERPARAMS[\"SAVED_MODEL_NAME\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c37470",
   "metadata": {},
   "source": [
    "**OR** run Training blocks to run initial exploration stage, for our agent to get an understanding of the game, and then the Training. Exploration is not exactly understanding, but is a way for us to fill the replay memory with images of the game being played with random actions.\n",
    "\n",
    "### Game Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e7c21e1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE = 1\n",
    "TRAIN   = 2\n",
    "TEST    = 3\n",
    "\n",
    "def slow_start(env,image_stack=[], rgb_stack=[],NOOPMAX=10):\n",
    "    idle_times = random.randint(4, NOOPMAX)\n",
    "    \n",
    "    #print(f'Agent is Staying Idle for {idle_times} times. Agent is thinking about what move to make...')\n",
    "    for idle_time in range(idle_times):\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]==False: env.render() # NOTE: Comment this in Google Colab\n",
    "        state, reward, done, info = env.step(0) # Zero means: NOOP or No Operation\n",
    "        processed_frame = preprocess(state)\n",
    "        image_stack.append(processed_frame)\n",
    "        rgb_stack.append(preprocess_rgb(state))\n",
    "        \n",
    "    return image_stack, rgb_stack\n",
    "\n",
    "def gameplay(PLAY_TYPE=TEST,MAX_EPISODE_PLAYTIME=1000000):\n",
    "    global FRAME_COUNT\n",
    "    #print(f'Agent is starting a new game: {e} games played.')\n",
    "    \n",
    "    # Reset Game\n",
    "    state = env.reset()\n",
    "    times_rewarded = times_penalized = 0\n",
    "    last_lives = 5\n",
    "    terminal_life_lost = False # False if last_lives==0 else True\n",
    "    \n",
    "    #print('Agent has made the start move.')\n",
    "    # Start the game by 'FIRE' action, incase if it doesnot start the game without it\n",
    "    state, _, _, _ = env.step(1) \n",
    "    \n",
    "    # Fill agent's memory with random times of no operation played\n",
    "    image_stack,rgb_stack = slow_start(env=env, NOOPMAX=HYPERPARAMS[\"NOOPMAX\"])\n",
    "    \n",
    "    \n",
    "    #print(f'Agent is now playing the game...')\n",
    "    i, state = 0, np.stack(image_stack[-4:], axis = 2)\n",
    "    while i < MAX_EPISODE_PLAYTIME:\n",
    "        agent.epsilon = 1  # Agent is Exploring the game by default\n",
    "        \n",
    "        # If agent has lost a life then start the game with 'FIRE' again.\n",
    "        if(terminal_life_lost == True):\n",
    "            state, _, _, _ = env.step(1) # 'FIRE' to start the game\n",
    "            slow_start(env, image_stack, rgb_stack, HYPERPARAMS[\"NOOPMAX\"])\n",
    "            state = np.stack(image_stack[-4:], axis = 2)\n",
    "\n",
    "        FRAME_COUNT = FRAME_COUNT + 1\n",
    "        action = env.action_space.sample() if PLAY_TYPE==EXPLORE else (agent.next_action(state) if PLAY_TYPE==TRAIN else np.argmax(agent.model.predict([np.expand_dims(state,axis=0)])[0]))\n",
    "\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]==False: env.render() # NOTE: Comment this in Google Colab\n",
    "        # Agent Makes random moves here...\n",
    "        action = action if action < env.action_space.n else env.action_space.sample()\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        rgb_stack.append(preprocess_rgb(next_state))\n",
    "        \n",
    "        # Agent updates it's game status here...\n",
    "        terminal_life_lost = True if info[HYPERPARAMS[\"LIVES\"]] < last_lives else done\n",
    "        last_lives = info[HYPERPARAMS[\"LIVES\"]]\n",
    "        \n",
    "            \n",
    "        if reward > 0:      times_rewarded = times_rewarded + 1\n",
    "        elif reward < 0: times_penalized = times_penalized + 1\n",
    "        elif terminal_life_lost: times_penalized = times_penalized + 1\n",
    "        # Making the starting experience of rewards more fruitful. For our replay memory...\n",
    "        reward = 10 if reward > 0 else (-30 if reward < 0 else reward)\n",
    "        reward = -30 if terminal_life_lost else reward\n",
    "        \n",
    "        # Store the stack of images for new a experience\n",
    "        processed_frame = preprocess(next_state)\n",
    "        image_stack = image_stack[-3:]\n",
    "        image_stack.append(processed_frame)\n",
    "        \n",
    "        next_state = np.stack(image_stack[-4:], axis = 2)\n",
    "        if(len(image_stack) != 4): print(\"Something's not right!! The stack size is less than expected.\")\n",
    "            \n",
    "        #Store experience in replay mem\n",
    "        if(PLAY_TYPE==EXPLORE or PLAY_TYPE==TRAIN): \n",
    "            agent.store_experience(state, action, reward, next_state, terminal_life_lost)\n",
    "        state = next_state\n",
    "        \n",
    "        if done: break\n",
    "        i+=1\n",
    "        \n",
    "    REWARD_HISTORY.append(times_rewarded)\n",
    "    return image_stack, times_rewarded, times_penalized, rgb_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1bef6",
   "metadata": {},
   "source": [
    "### TRAINING STAGE\n",
    "This step is recommended to be executed on Google Colab or over machine with GPU.\n",
    "\n",
    "**CONVERT THE BLOCKS to Code block. BELOW for TRAINING.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e84041",
   "metadata": {},
   "source": [
    "#### Initial Exploration stage\n",
    "This process takes around 3 min 28 seconds for Breakout on COLAB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c71d5e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\miniconda3\\envs\\cs677\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished exploring for 0 episodes\n",
      "Total Times Rewarded: 0, Total Times Penalized: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22232/744786292.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtotal_times_rewarded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_times_penalized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"NUM_EXPLORE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mimage_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes_rewarded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes_penalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgb_stack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgameplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEXPLORE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_EPISODE_PLAYTIME\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtotal_times_rewarded\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtotal_times_rewarded\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimes_rewarded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtotal_times_penalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_times_penalized\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mtimes_penalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22232/2860547221.py\u001b[0m in \u001b[0;36mgameplay\u001b[1;34m(PLAY_TYPE, MAX_EPISODE_PLAYTIME)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# Agent Makes random moves here...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mrgb_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_rgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    132\u001b[0m             \u001b[0mnum_steps\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnp_random\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrandint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mframeskip\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mact\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    151\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 152\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    153\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    154\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "'''\n",
    "    THIS IS DONE TO POPULATE REPLAY MEMORY WITH COMPLETE EXPLORATION TO INITIALIZE THE MEMORY \n",
    "'''\n",
    "total_times_rewarded=total_times_penalized=0\n",
    "for e in range(HYPERPARAMS[\"NUM_EXPLORE\"]):\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=EXPLORE, MAX_EPISODE_PLAYTIME=1000)\n",
    "    total_times_rewarded  = total_times_rewarded + times_rewarded\n",
    "    total_times_penalized = total_times_penalized+ times_penalized\n",
    "    if(e % 100 == 0): \n",
    "        print(\"Finished exploring for {} episodes\".format(e))\n",
    "        print(\"Total Times Rewarded: {}, Total Times Penalized: {}\".format(total_times_rewarded, total_times_penalized))\n",
    "\n",
    "print(\"EXPLORATION STEP COMPLETED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c4b1e7c",
   "metadata": {},
   "source": [
    "#### Train using DQN\n",
    "Every 100 episodes takes around 11-13 min for Breakout."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1db8d6ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Target model swapped successfully with Trained model!\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Target model swapped successfully with Trained model!\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Target model swapped successfully with Trained model!\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Target model swapped successfully with Trained model!\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Target model swapped successfully with Trained model!\n",
      "Agent is ready for gameplay...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22232/2203501354.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"NUM_EPISODES\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Agent is ready for gameplay...\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mimage_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes_rewarded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes_penalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgb_stack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgameplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTRAIN\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_EPISODE_PLAYTIME\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mrew_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimes_rewarded\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_22232/2860547221.py\u001b[0m in \u001b[0;36mgameplay\u001b[1;34m(PLAY_TYPE, MAX_EPISODE_PLAYTIME)\u001b[0m\n\u001b[0;32m     51\u001b[0m         \u001b[1;31m# Agent Makes random moves here...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     52\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0maction\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mn\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 53\u001b[1;33m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     54\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     55\u001b[0m         \u001b[0mrgb_stack\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpreprocess_rgb\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\gym\\wrappers\\time_limit.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     16\u001b[0m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m         ), \"Cannot call env.step() before calling reset()\"\n\u001b[1;32m---> 18\u001b[1;33m         \u001b[0mobservation\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_elapsed_steps\u001b[0m \u001b[1;33m>=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_max_episode_steps\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, a)\u001b[0m\n\u001b[0;32m    133\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnum_steps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m             \u001b[0mreward\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mact\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 135\u001b[1;33m         \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_obs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    136\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgame_over\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m\"ale.lives\"\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlives\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_get_obs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    151\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_obs_type\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"image\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 153\u001b[1;33m             \u001b[0mimg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    154\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    155\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36m_get_image\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    139\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_image\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 140\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0male\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    141\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_get_ram\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\atari_py\\ale_python_interface.py\u001b[0m in \u001b[0;36mgetScreenRGB2\u001b[1;34m(self, screen_data)\u001b[0m\n\u001b[0;32m    264\u001b[0m             \u001b[0mscreen_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muint8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    265\u001b[0m         \u001b[1;32massert\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrides\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 266\u001b[1;33m         \u001b[0male_lib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgetScreenRGB2\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mas_ctypes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscreen_data\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    267\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mscreen_data\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('LOSS')\n",
    "\n",
    "'''\n",
    "TRAIN DQN ON THE FIRST GAME\n",
    "'''\n",
    "rew_list = []\n",
    "e=0\n",
    "for e in range(1, HYPERPARAMS[\"NUM_EPISODES\"]+1):\n",
    "    print(\"Agent is ready for gameplay...\")\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=TRAIN, MAX_EPISODE_PLAYTIME=1000)\n",
    "            \n",
    "    rew_list.append(times_rewarded)\n",
    "            \n",
    "    if(e % HYPERPARAMS[\"AUTOSAVE_CHECKPOINT\"] == 0):\n",
    "        clear_output(wait=True)\n",
    "        print(\"Finished episode \", e , \"/\", HYPERPARAMS[\"NUM_EPISODES\"], \" Total reward = \", sum(rew_list)/len(rew_list), \" eps = \", agent.epsilon)\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]:\n",
    "            if not os.path.exists(HYPERPARAMS[\"VIS_DIR\"]):\n",
    "                os.mkdir(HYPERPARAMS[\"VIS_DIR\"])\n",
    "            generate_gif(len(rgb_stack), rgb_stack, sum(rew_list)/(len(rew_list)), HYPERPARAMS[\"VIS_DIR\"] + \"/\", e)\n",
    "        \n",
    "        rew_list = []\n",
    "        print(f\"Saving Model Checkpoint at episode {e}...\")\n",
    "        agent.save_model(\"models/tmp_model_breakout_\" + str(e) + \".h5\")        \n",
    "        ax.set_xlim(0,e)\n",
    "        ax.set_ylim(0,max(LOSS_HISTORY))\n",
    "        plt.plot(LOSS_HISTORY)\n",
    "        plt.show()\n",
    "\n",
    "    # BACKPROP INITIATED AT THE END OF EVERY EPISODE AND NOT AT TGT_FREQ\n",
    "    agent.replay(HYPERPARAMS[\"MINIBATCH_SIZE\"], model_swap)\n",
    "    model_swap = model_swap + 1\n",
    "    \n",
    "print(f\"Agent is now prepared now, after training for {e} episodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e1414",
   "metadata": {},
   "source": [
    "#### Saving the final model"
   ]
  },
  {
   "cell_type": "raw",
   "id": "15851332",
   "metadata": {},
   "source": [
    "agent.save_model(agent.model, \"models/\" + HYPERPARAMS[\"SAVED_MODEL_NAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512222e",
   "metadata": {},
   "source": [
    "## Testing the model\n",
    "If you open checkpoint \"model_dqn_breakout_1.h5\" we can see that model is able to move towards the ball direction, but still has to keep up to get the score. This is some progress from random movement from random sample of env.action_space.n. We have achieved this model about 18hrs of training on GOOGLE COLAB in the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ff2c1e67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the agent ...\n",
      "Agent has been Sucessfully setup ...\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(HYPERPARAMS[\"ENV_NAME\"])\n",
    "agent = Agent(env, model_pathname=\"models/model_dqn_breakout_1.h5\" ) # model_pathname=(\"models/\" + HYPERPARAMS[\"SAVED_MODEL_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3c627446",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " image (InputLayer)          [(None, 84, 84, 4)]       0         \n",
      "                                                                 \n",
      " conv2D_1 (Conv2D)           (None, 20, 20, 32)        8192      \n",
      "                                                                 \n",
      " conv2D_2 (Conv2D)           (None, 9, 9, 64)          32768     \n",
      "                                                                 \n",
      " conv2D_3 (Conv2D)           (None, 7, 7, 64)          36864     \n",
      "                                                                 \n",
      " conv2D_4 (Conv2D)           (None, 1, 1, 1024)        3211264   \n",
      "                                                                 \n",
      " flat_1 (Flatten)            (None, 1024)              0         \n",
      "                                                                 \n",
      " q_values (Dense)            (None, 4)                 4100      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,293,188\n",
      "Trainable params: 3,293,188\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "agent.model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "aa84b109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0 / 20  Total reward =  10\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15800/589837055.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtotal_times_rewarded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_times_penalized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"NUM_EVAL\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m     \u001b[0mimage_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes_rewarded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes_penalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgb_stack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgameplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTEST\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_EPISODE_PLAYTIME\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mtotal_times_rewarded\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtotal_times_rewarded\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimes_rewarded\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_15800/2860547221.py\u001b[0m in \u001b[0;36mgameplay\u001b[1;34m(PLAY_TYPE, MAX_EPISODE_PLAYTIME)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     47\u001b[0m         \u001b[0mFRAME_COUNT\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mFRAME_COUNT\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 48\u001b[1;33m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mEXPLORE\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mTRAIN\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     49\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     50\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ON_COLAB\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# NOTE: Comment this in Google Colab\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1783\u001b[0m       \u001b[0mcallbacks\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mon_predict_begin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1784\u001b[0m       \u001b[0mbatch_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1785\u001b[1;33m       \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterator\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menumerate_epochs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Single epoch.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1786\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcatch_stop_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1787\u001b[0m           \u001b[1;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36menumerate_epochs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1193\u001b[0m     \u001b[1;34m\"\"\"Yields `(epoch, tf.data.Iterator)`.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1194\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_truncate_execution_to_epoch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1195\u001b[1;33m       \u001b[0mdata_iterator\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0miter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1196\u001b[0m       \u001b[1;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_initial_epoch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_epochs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1197\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_insufficient_data\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# Set by `catch_stop_iteration`.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    488\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minside_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolocate_with\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_variant_tensor\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 490\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0miterator_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mOwnedIterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    491\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m       raise RuntimeError(\"`tf.data.Dataset` only supports Python-style \"\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, dataset, components, element_spec)\u001b[0m\n\u001b[0;32m    724\u001b[0m             \u001b[1;34m\"When `dataset` is provided, `element_spec` and `components` must \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    725\u001b[0m             \"not be specified.\")\n\u001b[1;32m--> 726\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    727\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    728\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_next_call_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\data\\ops\\iterator_ops.py\u001b[0m in \u001b[0;36m_create_iterator\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    749\u001b[0m               \u001b[0moutput_types\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_flat_output_types\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    750\u001b[0m               output_shapes=self._flat_output_shapes))\n\u001b[1;32m--> 751\u001b[1;33m       \u001b[0mgen_dataset_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_iterator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mds_variant\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterator_resource\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    752\u001b[0m       \u001b[1;31m# Delete the resource when this object is deleted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    753\u001b[0m       self._resource_deleter = IteratorResourceDeleter(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\ops\\gen_dataset_ops.py\u001b[0m in \u001b[0;36mmake_iterator\u001b[1;34m(dataset, iterator, name)\u001b[0m\n\u001b[0;32m   3235\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3236\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[1;32m-> 3237\u001b[1;33m         _ctx, \"MakeIterator\", name, dataset, iterator)\n\u001b[0m\u001b[0;32m   3238\u001b[0m       \u001b[1;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3239\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "rew_list = []\n",
    "total_times_rewarded=total_times_penalized=0\n",
    "for e in range(HYPERPARAMS[\"NUM_EVAL\"]):\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=TEST, MAX_EPISODE_PLAYTIME=1000)\n",
    "    \n",
    "    total_times_rewarded  = total_times_rewarded + times_rewarded\n",
    "    total_times_penalized = total_times_penalized+ times_penalized\n",
    "    rew_list.append(times_rewarded)\n",
    "    \n",
    "    print(\"Finished episode \", e , \"/\", HYPERPARAMS[\"NUM_EVAL\"], \" Total reward = \", times_rewarded*(10))\n",
    "    if HYPERPARAMS[\"ON_COLAB\"]:\n",
    "        if not os.path.exists(\"test\"):\n",
    "            os.mkdir(\"test\")\n",
    "        generate_gif(len(rgb_stack), rgb_stack, total_times_rewarded, \"test/\", e)\n",
    "\n",
    "print(\"Testing Complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ecc8b9e",
   "metadata": {},
   "source": [
    "## Transfer Learning: PART II"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2853d2a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the agent ...\n",
      "Agent has been Sucessfully setup ...\n"
     ]
    }
   ],
   "source": [
    "prev_env = gym.make('BreakoutDeterministic-v4')\n",
    "env = gym.make('SpaceInvaders-v0')\n",
    "\n",
    "agent = Agent(prev_env) # Using Previous model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9b1a44bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transfer Learning condition: Requires (prev_env.action_space.n <= env.action_space.n)\n",
    "# We are assuming the next game to be played is higher complex than old game we have trained.\n",
    "agent.transfer_learning(\n",
    "    env,\n",
    "    'models/model_dqn_breakout_1.h5', # HYPERPARAMS[\"TRANSFER_MODEL_NAME\"],\n",
    "    agent.action_size, # Number of Old actions=4\n",
    "    (env.action_space.n-agent.action_size) # Number of New actions, Excluding old one's = 2\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b2459ab",
   "metadata": {},
   "source": [
    "### Training (On new Game using model of old game)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1e7a79a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is ready for gameplay...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\miniconda3\\envs\\cs677\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game Play Paused! Model is training on it's past Memory\n",
      "Target model swapped successfully with Trained model!\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Target model swapped successfully with Trained model!\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Target model swapped successfully with Trained model!\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19964/315837150.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     37\u001b[0m     \u001b[1;31m# BACKPROP INITIATED AT THE END OF EVERY EPISODE AND NOT AT TGT_FREQ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 38\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"MINIBATCH_SIZE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_swap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     39\u001b[0m     \u001b[0mmodel_swap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_swap\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_19964/2716918778.py\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, batch_size, model_swap)\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[1;31m# START TRAINING PROCESS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m             \u001b[1;31m# Get the loss between NoAction and the Expected Action that model suplies\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m             \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0my_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"RETRAIN\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# Append loss to it's history, only on the last re-train loop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[0;32m   1898\u001b[0m                                                     class_weight)\n\u001b[0;32m   1899\u001b[0m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1900\u001b[1;33m       \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1901\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1902\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msync_to_numpy_or_python_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    149\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 150\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    151\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 910\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    911\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    912\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    940\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    941\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 942\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    943\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    944\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3129\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m   3130\u001b[0m     return graph_function._call_flat(\n\u001b[1;32m-> 3131\u001b[1;33m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0m\u001b[0;32m   3132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3133\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1958\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1959\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1960\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1961\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1962\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    601\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    602\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 603\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    604\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    605\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\cs677\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[1;32m---> 59\u001b[1;33m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[0;32m     60\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYoAAAEKCAYAAAAMzhLIAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAARO0lEQVR4nO3dfYxldX3H8fcHkEp5Et0lJewKqIu6RQWcUBpMxWKbhbZLg1bYllgskWjF1oeaYrWImP6hxodQ1+KKiNoKrjYl24DFBDC0ylqG8CC7BLsgwiIJiyK0QXnQb/+4B+c6zP5mdtgz9+7u+5VM9pzf+d1zv/vLzHzmPP1uqgpJkrZmt1EXIEkabwaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaeguKJBcneSDJbVvZniQXJNmU5NYkR/dViyRp/vo8orgEWNHYfiKwrPs6C/inHmuRJM1Tb0FRVdcBP250ORn4Yg2sB56T5KC+6pEkzc8eI3zvg4F7h9Y3d233T++Y5CwGRx3svffer3zJS16yIAVK0s7ixhtvfLCqFs/ntaMMijmrqjXAGoCJiYmanJwccUWStGNJ8oP5vnaUdz3dBywdWl/StUmSxsgog2Id8Mbu7qdjgYer6mmnnSRJo9XbqacklwLHA4uSbAY+ADwLoKouBK4ETgI2AY8Cb+qrFknS/PUWFFW1apbtBbytr/eXJG0fPpktSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpqdegSLIiyR1JNiU5Z4btz09ybZKbktya5KQ+65EkbbvegiLJ7sBq4ERgObAqyfJp3d4PrK2qo4DTgE/3VY8kaX76PKI4BthUVXdV1ePAZcDJ0/oUsF+3vD/wwx7rkSTNQ59BcTBw79D65q5t2HnA6Uk2A1cCb59pR0nOSjKZZHLLli191CpJ2opRX8xeBVxSVUuAk4AvJXlaTVW1pqomqmpi8eLFC16kJO3K+gyK+4ClQ+tLurZhZwJrAarqeuDZwKIea5IkbaM+g+IGYFmSw5LsyeBi9bppfe4BTgBI8lIGQeG5JUkaI70FRVU9CZwNXAXczuDupg1Jzk+ysuv2buDNSW4BLgXOqKrqqyZJ0rbbo8+dV9WVDC5SD7edO7S8ETiuzxokSc/MqC9mS5LGnEEhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlq6jUokqxIckeSTUnO2UqfNyTZmGRDki/3WY8kadvt0deOk+wOrAZ+D9gM3JBkXVVtHOqzDHgvcFxVPZTkwL7qkSTNT59HFMcAm6rqrqp6HLgMOHlanzcDq6vqIYCqeqDHeiRJ89BnUBwM3Du0vrlrG3Y4cHiSbyVZn2TFTDtKclaSySSTW7Zs6alcSdJMRn0xew9gGXA8sAr4bJLnTO9UVWuqaqKqJhYvXrywFUrSLq7PoLgPWDq0vqRrG7YZWFdVT1TV94HvMQgOSdKY6DMobgCWJTksyZ7AacC6aX0uZ3A0QZJFDE5F3dVjTZKkbdRbUFTVk8DZwFXA7cDaqtqQ5PwkK7tuVwE/SrIRuBZ4T1X9qK+aJEnbLlU16hq2ycTERE1OTo66DEnaoSS5saom5vPaUV/MliSNOYNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqakZFEn+KMkhQ+vnJrklybokh/VfniRp1GY7ovgHYAtAkj8ETgf+gsFUHBf2W5okaRzMFhRVVY92y6cAn6uqG6vqIsBpXCVpFzBbUCTJPkl2A04Arh7a9uz+ypIkjYvZPgr1k8DNwCPA7VU1CZDkKOD+XiuTJI2FZlBU1cVJrgIOBG4Z2nQ/8KY+C5MkjYdmUHR3PP2kqu7r1l8D/DHwA+BTvVcnSRq52a5RrAX2BkhyJPBV4B7gFcCne61MkjQWZrtGsVdV/bBbPh24uKo+1l3cvrnXyiRJY2HWu56Gln+X7q6nqvpFbxVJksbKbEcU1yRZy+Di9QHANQBJDgIe77k2SdIYmC0o3gGcChwEvKqqnujafwN4X491SZLGxGy3xxZwWTev01Hd8xMbq+qmBalOkjRys90eux9wEfBKpp6jODLJjcCZVfVIz/VJkkZstovZFwAbgWVVdUpVnQK8EPguPkchSbuE2a5RHFdVZww3dKejzk/yP71VJUkaG8/kg4syexdJ0o5utqD4dvdhRb8SCkn+Hri+v7IkSeNitlNPbwc+B2xKcnPXdiRwE3Bmf2VJksbFbLfHPgL8SZIXAsu75o1VdWeSdzCYhlyStBOb7YgCgKq6E7hzWvO7MCgkaafnxWxJUtMzCYrablVIksbWbE9m/y8zB0KAvXqpSJI0Vma7mL3vQhUiSRpPz+TUkyRpF2BQSJKaDApJUlOvQZFkRZI7kmxKck6j3+uSVJKJPuuRJG273oIiye7AauBEBk91r0qyfIZ++wJ/DXynr1okSfPX5xHFMcCmqrqrqh4HLgNOnqHfh4APAz/rsRZJ0jz1GRQHA/cOrW/u2n4pydHA0qq6orWjJGclmUwyuWXLlu1fqSRpq0Z2MTvJbsDHgXfP1req1lTVRFVNLF68uP/iJEm/1GdQ3AcsHVpf0rU9ZV/gCOCbSe4GjgXWeUFbksZLn0FxA7AsyWFJ9gROA9Y9tbGqHq6qRVV1aFUdCqwHVlbVZI81SZK2UW9BUVVPAmcDVwG3A2urakOS85Os7Ot9JUnb15w+j2K+qupK4Mppbedupe/xfdYiSZofn8yWJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpKZegyLJiiR3JNmU5JwZtr8rycYktya5OskhfdYjSdp2vQVFkt2B1cCJwHJgVZLl07rdBExU1cuBrwEf6aseSdL89HlEcQywqaruqqrHgcuAk4c7VNW1VfVot7oeWNJjPZKkeegzKA4G7h1a39y1bc2ZwNdn2pDkrCSTSSa3bNmyHUuUJM1mLC5mJzkdmAA+OtP2qlpTVRNVNbF48eKFLU6SdnF79Ljv+4ClQ+tLurZfkeS1wPuAV1fVYz3WI0mahz6PKG4AliU5LMmewGnAuuEOSY4CPgOsrKoHeqxFkjRPvQVFVT0JnA1cBdwOrK2qDUnOT7Ky6/ZRYB/gq0luTrJuK7uTJI1In6eeqKorgSuntZ07tPzaPt9fkvTMjcXFbEnS+DIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKnJoJAkNRkUkqQmg0KS1GRQSJKaDApJUpNBIUlqMigkSU0GhSSpyaCQJDUZFJKkJoNCktRkUEiSmgwKSVKTQSFJajIoJElNBoUkqcmgkCQ1GRSSpCaDQpLUZFBIkpoMCklSk0EhSWoyKCRJTQaFJKmp16BIsiLJHUk2JTlnhu2/luQr3fbvJDm0z3okSduut6BIsjuwGjgRWA6sSrJ8WrczgYeq6kXAJ4AP91WPJGl++jyiOAbYVFV3VdXjwGXAydP6nAx8oVv+GnBCkvRYkyRpG+3R474PBu4dWt8M/NbW+lTVk0keBp4HPDjcKclZwFnd6mNJbuul4h3PIqaN1S7MsZjiWExxLKa8eL4v7DMotpuqWgOsAUgyWVUTIy5pLDgWUxyLKY7FFMdiSpLJ+b62z1NP9wFLh9aXdG0z9kmyB7A/8KMea5IkbaM+g+IGYFmSw5LsCZwGrJvWZx3w593y64Frqqp6rEmStI16O/XUXXM4G7gK2B24uKo2JDkfmKyqdcDngC8l2QT8mEGYzGZNXzXvgByLKY7FFMdiimMxZd5jEf+AlyS1+GS2JKnJoJAkNY1tUDj9x5Q5jMW7kmxMcmuSq5McMoo6F8JsYzHU73VJKslOe2vkXMYiyRu6740NSb680DUulDn8jDw/ybVJbup+Tk4aRZ19S3Jxkge29qxZBi7oxunWJEfPacdVNXZfDC5+3wm8ANgTuAVYPq3PXwIXdsunAV8Zdd0jHIvXAL/eLb91Vx6Lrt++wHXAemBi1HWP8PtiGXATcEC3fuCo6x7hWKwB3totLwfuHnXdPY3F7wBHA7dtZftJwNeBAMcC35nLfsf1iMLpP6bMOhZVdW1VPdqtrmfwzMrOaC7fFwAfYjBv2M8WsrgFNpexeDOwuqoeAqiqBxa4xoUyl7EoYL9ueX/ghwtY34KpqusY3EG6NScDX6yB9cBzkhw0237HNShmmv7j4K31qaongaem/9jZzGUshp3J4C+GndGsY9EdSi+tqisWsrARmMv3xeHA4Um+lWR9khULVt3CmstYnAecnmQzcCXw9oUpbexs6+8TYAeZwkNzk+R0YAJ49ahrGYUkuwEfB84YcSnjYg8Gp5+OZ3CUeV2Sl1XVT0ZZ1IisAi6pqo8l+W0Gz28dUVW/GHVhO4JxPaJw+o8pcxkLkrwWeB+wsqoeW6DaFtpsY7EvcATwzSR3MzgHu24nvaA9l++LzcC6qnqiqr4PfI9BcOxs5jIWZwJrAarqeuDZDCYM3NXM6ffJdOMaFE7/MWXWsUhyFPAZBiGxs56HhlnGoqoerqpFVXVoVR3K4HrNyqqa92RoY2wuPyOXMziaIMkiBqei7lrAGhfKXMbiHuAEgCQvZRAUWxa0yvGwDnhjd/fTscDDVXX/bC8ay1NP1d/0HzucOY7FR4F9gK921/PvqaqVIyu6J3Mci13CHMfiKuD3k2wEfg68p6p2uqPuOY7Fu4HPJnkngwvbZ+yMf1gmuZTBHweLuusxHwCeBVBVFzK4PnMSsAl4FHjTnPa7E46VJGk7GtdTT5KkMWFQSJKaDApJUpNBIUlqMigkSU0GhXZJSX6e5Oahr63ORNv1f0uSN26H9727e6ZB2mF4e6x2SUn+r6r2GcH73s1gRtsHF/q9pfnyiEIa0v3F/5Ek303y30le1LWfl+RvuuW/Gvr8j8u6tucmubxrW5/k5V3785J8o/s8iIsYTO/81Hud3r3HzUk+k2T37uuSJLd1NbxzBMMg/QqDQruqvaadejp1aNvDVfUy4FPAJ2d47TnAUVX1cuAtXdsHgZu6tr8Dvti1fwD4r6r6TeDfgOfDL6eROBU4rqqOZPDk9J8BRwIHV9URXQ2f317/YWm+xnIKD2kB/LT7BT2TS4f+/cQM228F/iXJ5QzmUwJ4FfA6gKq6pjuS2I/BB8mc0rVfkeShrv8JwCuBG7ppV/YCHgD+HXhBkn8ErgC+Mc//n7TdeEQhPV1tZfkpfwCsZvBJYjd0sxdvqwBfqKoju68XV9V53YcMvQL4JoOjlYvmsW9puzIopKc7dejf64c3dJ95sbSqrgX+lsH09vsA/8ng1BFJjgcerKpHGHwk65927ScCB3S7uhp4fZIDu23PTXJId0fUblX1r8D7GYSRNFKeetKuaq8kNw+t/0dVPXWL7AFJbgUeY/CBN8N2B/45yf4MjgouqKqfJDkPuLh73aNMTYH/QeDSJBuAbzOY7pqq2pjk/cA3uvB5Angb8FPg810bwHu32/9Ymidvj5WGePuq9HSeepIkNXlEIUlq8ohCktRkUEiSmgwKSVKTQSFJajIoJElN/w/KGbzGmWlBnwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "%matplotlib inline\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "fig,ax = plt.subplots(1,1)\n",
    "ax.set_xlabel('Episodes')\n",
    "ax.set_ylabel('LOSS')\n",
    "\n",
    "\n",
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "'''\n",
    "TRAIN DQN ON THE SECOND GAME\n",
    "'''\n",
    "rew_list = []\n",
    "e=0\n",
    "for e in range(1, HYPERPARAMS[\"NUM_EPISODES\"]+1):\n",
    "    print(\"Agent is ready for gameplay...\")\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=TRAIN, MAX_EPISODE_PLAYTIME=1000)\n",
    "            \n",
    "    rew_list.append(times_rewarded)\n",
    "            \n",
    "    if(e % HYPERPARAMS[\"AUTOSAVE_CHECKPOINT\"] == 0):\n",
    "        print(\"Finished episode \", e , \"/\", HYPERPARAMS[\"NUM_EPISODES\"], \" Total reward = \", sum(rew_list)/len(rew_list), \" eps = \", agent.epsilon)\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]:\n",
    "            if not os.path.exists(HYPERPARAMS[\"VIS_DIR\"]):\n",
    "                os.mkdir(HYPERPARAMS[\"VIS_DIR\"])\n",
    "            generate_gif(len(rgb_stack), rgb_stack, sum(rew_list)/(len(rew_list)), HYPERPARAMS[\"VIS_DIR\"] + \"/\", e)\n",
    "        \n",
    "        rew_list = []\n",
    "        print(f\"Saving Model Checkpoint at episode {e}...\")\n",
    "        agent.save_model(\"models/tmp_model_spaceinvader_\" + str(e) + \".h5\")\n",
    "        ax.set_xlim(0,e)\n",
    "        ax.set_ylim(0,max(LOSS_HISTORY))\n",
    "        plt.plot(LOSS_HISTORY)\n",
    "        plt.show()\n",
    "\n",
    "    # BACKPROP INITIATED AT THE END OF EVERY EPISODE AND NOT AT TGT_FREQ\n",
    "    agent.replay(HYPERPARAMS[\"MINIBATCH_SIZE\"], model_swap)\n",
    "    model_swap = model_swap + 1\n",
    "    \n",
    "print(f\"Agent is now prepared now, after training for {e} episodes.\")\n",
    "\n",
    "print(\"Transfer Learning's Training Completes\")\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bef9558",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model('./models/model_dqn_spaceinvader.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8cba1e3",
   "metadata": {},
   "source": [
    "If you have conducted any training the Loss History can be showcased w.r.t episodes trained using below code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cee3a180",
   "metadata": {},
   "source": [
    "### Testing\n",
    "\n",
    "Initially if you run this with the original 'dqn_model.h5' you can see that the agent forgot how to FIRE in SpaceInvader. Because, the fire action is only used once throught Breakout, at Start. Also we can see that the agent is moving close to the bullet at time, as it is assuming bullets of SpaceInvader is the ball it was trained to save from dropping in Breakout. Now all we have to do is Train the model of SpaceInvader from here by using above code block for training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e3513dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0 / 20  Total reward =  0\n",
      "Finished episode  1 / 20  Total reward =  0\n",
      "Finished episode  2 / 20  Total reward =  0\n",
      "Finished episode  3 / 20  Total reward =  0\n",
      "Finished episode  4 / 20  Total reward =  0\n",
      "Finished episode  5 / 20  Total reward =  0\n",
      "Finished episode  6 / 20  Total reward =  0\n",
      "Finished episode  7 / 20  Total reward =  0\n",
      "Finished episode  8 / 20  Total reward =  0\n",
      "Finished episode  9 / 20  Total reward =  0\n",
      "Finished episode  10 / 20  Total reward =  0\n",
      "Finished episode  11 / 20  Total reward =  0\n",
      "Finished episode  12 / 20  Total reward =  0\n",
      "Finished episode  13 / 20  Total reward =  0\n",
      "Finished episode  14 / 20  Total reward =  0\n",
      "Finished episode  15 / 20  Total reward =  0\n",
      "Finished episode  16 / 20  Total reward =  0\n",
      "Finished episode  17 / 20  Total reward =  10\n",
      "Finished episode  18 / 20  Total reward =  0\n",
      "Finished episode  19 / 20  Total reward =  0\n",
      "Transfer Learning's Testing Completes\n",
      "Duration: 0:09:25.716924\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "start_time = datetime.now()\n",
    "\n",
    "rew_list = []\n",
    "total_times_rewarded=total_times_penalized=0\n",
    "for e in range(HYPERPARAMS[\"NUM_EVAL\"]):\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=TEST, MAX_EPISODE_PLAYTIME=1000)\n",
    "    \n",
    "    total_times_rewarded  = total_times_rewarded + times_rewarded\n",
    "    total_times_penalized = total_times_penalized+ times_penalized\n",
    "    rew_list.append(times_rewarded)\n",
    "    \n",
    "    print(\"Finished episode \", e , \"/\", HYPERPARAMS[\"NUM_EVAL\"], \" Total reward = \", times_rewarded*(10))\n",
    "    if HYPERPARAMS[\"ON_COLAB\"]:\n",
    "        if not os.path.exists(\"test\"):\n",
    "            os.mkdir(\"test\")\n",
    "        generate_gif(len(rgb_stack), rgb_stack, total_times_rewarded, \"test/\", e)\n",
    "\n",
    "print(\"Transfer Learning's Testing Completes\")\n",
    "end_time = datetime.now()\n",
    "print('Duration: {}'.format(end_time - start_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53abaf4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "exit(1) # Close Gym Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb40f02",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
