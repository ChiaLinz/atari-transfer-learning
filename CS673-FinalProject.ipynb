{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "71d62341",
   "metadata": {},
   "source": [
    "# CS673 Deep Learning | Final Project\n",
    "\n",
    "## Proposal -  Improved Bot Learning process on Atari games by using Transfer Learning\n",
    "\n",
    "## Team: \n",
    "- Ching-Hao Sun\n",
    "- Chia-Lin Hsieh\n",
    "- Rahul Gautham Putcha\n",
    "\n",
    "## Index\n",
    "- [Abstract](#Abstract)\n",
    "- [Baseline Models](#Baseline-Models:)\n",
    "- [Candidate ML Models / Methods](#Candidate-ML-Models-/-Methods)\n",
    "- [Project Environment Setup](#Project-Environment-Setup)\n",
    "  - [Part 1: Installation of GYM](#Part-1)\n",
    "  - [Part 2: Reinforcement Learning Dependencies](#Part-2)\n",
    "- [Working on the Project: PART I - Learning the first game](#Working-on-the-Project:-PART-I---Learning-the-first-game)\n",
    "  - [Hyperparameters](#Hyperparameters)\n",
    "  - [About Reinforcement Learning](#About-Reinforcement-Learning) : (Yet-to-update)\n",
    "  - [Replay Memory](#Replay-Memory)\n",
    "  - [Agent](#Agent)\n",
    "  - [Starting the Game Environment](#Starting-the-Game-Environment)\n",
    "  - [A short demo: Of how the model predicts](#A-short-demo:-Of-how-the-model-predicts)\n",
    "  - [Q-Learning](#Q-Learning)\n",
    "  - [Model Checkpointing](#Model-Checkpointing)\n",
    "- [Working on the Project: PART II - Learning the second game](#Working-on-the-Project:-PART-I---Learning-the-first-game)\n",
    "\n",
    "\n",
    "## Abstract\n",
    "Reinforcement learning algorithms require tens of thousands or millions of time steps -\n",
    "which is equivalent to several weeks of training in real time to learn how to play a\n",
    "single game. Having a bot trained from scratch is costly in terms of time and processing\n",
    "power.\n",
    "\n",
    "Suppose we have a pre-trained model of a bot that has already learnt to play one game. \n",
    "We intend to make use of the same trained-model for a bot in learning another game of \n",
    "a similar traits/environment, thereby improving the efficiency of learning the second \n",
    "game and expanding the botâ€™s knowledge in tackling multiple games in less time.\n",
    "\n",
    "## Baseline Models:\n",
    "CNN (Convolutional Neural Network) with DQN (Deep-Q-Network; a Q-Learning variant)\n",
    "\n",
    "\n",
    "## Candidate ML Models / Methods\n",
    "- Deep Convolutional Neural Network\n",
    "- Deep Q-Network\n",
    "- Transfer Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cc4ae2",
   "metadata": {},
   "source": [
    "## Project Environment Setup\n",
    "\n",
    "### Part 1\n",
    "#### Requirements:\n",
    "  - Development Environment Window (Installation procedure is similar for Linux and Mac too...)\n",
    "  - Miniconda or Anaconda with conda cmd installed \n",
    "\n",
    "\n",
    "#### Install Microsoft Visual Studio 2022 (For Windows only)\n",
    "  - Select Build Tools Desktop Development with C++\n",
    "\n",
    "#### Installation process\n",
    "- Open a Terminal (For example: Command prompt) ... Require Conda cmd installed by means of Miniconda installation setup\n",
    "- Setup a new environment **(Recommended)**\\\n",
    "    <code>$ conda create -n env3</code>\n",
    "    \n",
    "    <code>$ conda activate env3</code>\n",
    "\n",
    "- Install Necessary Package in our new environments\n",
    "\n",
    "  - Install Python3.7 \\\n",
    "    <code>$ conda install python=3.7</code>\n",
    "    \n",
    "  - Install OpenAI Gym for Atari games \\\n",
    "    <code>$ pip install gym[atari]</code>\n",
    "\n",
    "- With this project comes a git repository where you can download the project folder structure and the necessary file after environment setup shown above.\n",
    "   - The link to [GitHub project repo](https://github.com)\n",
    "\n",
    "\n",
    "- After Setting up the repo locally into your computer, put all of your atari game into the './roms' folder.\n",
    "- Choose a Atari game from any of the following sources or your choice:\n",
    "  - [Breakout from oldgames.sk](https://www.oldgames.sk/en/game/breakout/download/8314/)\n",
    "  - [SpaceInvaders from consoleroms.com](https://www.consoleroms.com/roms/atari-2600/space-invaders)\n",
    "  - [SpaceInvaders from atarimania.com](http://www.atarimania.com/game-atari-2600-vcs-space-invaders_s6947.html)\n",
    "\n",
    "- Also, you can see by default the roms folder contains Breakout and SpaceInvaders '.bin' files in it.\n",
    "- After putting all of you games that you want to run in this project, go back to the terminal where you are running conda environment.\n",
    "- Here, run following cmd to load in the game into our arcade learning environment (A way for us to use the atari games using open-ai gym) \\\n",
    "    <code>$ ale-import-roms /roms</code>\n",
    "\n",
    "**You are now all set to Run this project...**\n",
    "\n",
    "If not, no need to worry. Execute below project steps sequentially to get all dependencies setup in no time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184a5065",
   "metadata": {},
   "source": [
    "### Making Gym[Atari] work on our localhost\n",
    "At first we load the games by importing the Arcade Learning Environment package. we uploaded the games using ale-import-roms into this program and use it inside gym emulator. This is a setup tutorial, if you have already done with the setup feel free to skip and proceed to [Part 2](#Part-2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b28603",
   "metadata": {},
   "source": [
    "This project requires Python 3.7 and gym[atari]==0.19.0\n",
    "\n",
    "**Execute below line (START to END) if you are on Google Colab else execute below line in conda CLI environment such as env3 as mentioned above**\n",
    "\n",
    "For Example,\\\n",
    "Conda Terminal or CMD prompt(Windows) or Terminal(Linux or Mac OS)\\\n",
    "<code>(env3) path> conda uninstall python</code>\\\n",
    "<code>(env3) path> conda install python=3.7</code>\n",
    "\n",
    "**START**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "515d0ab9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!conda uninstall python\n",
    "!conda install python=3.7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bbd6b0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall gym\n",
    "!pip install gym[atari]==0.19.0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df016e9c",
   "metadata": {},
   "source": [
    "**END**\n",
    "\n",
    "**(Mandatary execution)** Executing below step will import games that are necessary for our project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66567d64",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "!ale-import-roms roms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ea1c7af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "from ale_py import ALEInterface\n",
    "from ale_py.roms import Breakout\n",
    "from ale_py.roms import SpaceInvaders\n",
    "\n",
    "ale = ALEInterface()        # Ignore any Deprecation warnings cause by this line\n",
    "ale.loadROM(Breakout)       # This line will load your Breakout game into this project\n",
    "ale.loadROM(SpaceInvaders)  # This line will load your SpaceInvaders game into this project"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e0dbe2",
   "metadata": {},
   "source": [
    "Let Try to see the Breakout atari game inside of gym,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41694f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make('Breakout-v4')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676e1f5e",
   "metadata": {},
   "source": [
    "Actions moves the bot can make in this game:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d936c5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"This game supports {env.action_space.n} action moves\")\n",
    "print(f\"The moves are {env.unwrapped.get_action_meanings()}\") # Note that the NOOP means no operation or no move"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dedd4cfd",
   "metadata": {},
   "source": [
    "A basic game play can be executed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f96f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset() # Start the game from beginning\n",
    "t=0 # timestamp (epoch)\n",
    "while True: # Run the game till the game is over, for every timestep\n",
    "    if HYPERPARAMS[\"ON_COLAB\"]==False: env.render() # Print the game to the screen ...\n",
    "    action = env.action_space.sample() # Random action\n",
    "    observation, reward, done, info = env.step(action) # At each step try random action\n",
    "    if done: # if the game is over (End of the game: can be win, lose or draw in any game) => Stop the game\n",
    "        print(\"Episode is finished after the {} timesteps\".format(t+1))\n",
    "        print(\"Episode info: {}\".format(info)) # What the reason? for the game to stop\n",
    "        break\n",
    "    t=t+1\n",
    "\n",
    "env.close() # Close the window\n",
    "print() # Just for format: This one just prints nothing so we can avoid the print of previous line in jupyter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0058e6b9",
   "metadata": {},
   "source": [
    "You will see the game window pop up and close automatically. If you did Hurray!! We are now able to work with any game using Gym in our project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ba82d35",
   "metadata": {},
   "source": [
    "### Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b8601b",
   "metadata": {},
   "source": [
    "Install Tensorflow and Keras,\n",
    "\n",
    "**Execute below line (START to END) if you are on Google Colab else execute below line in conda CLI environment such as env3 as mentioned above**\n",
    "\n",
    "**START**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "094772f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall keras==2.7.0\n",
    "!pip install keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3de2605",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip uninstall tensorflow\n",
    "!pip install --upgrade tensorflow==2.7.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9a8f341",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install numpy\n",
    "!pip install opencv-python\n",
    "!pip install pyglet\n",
    "!pip install scikit-image\n",
    "# For PIL\n",
    "!pip install pillow\n",
    "!pip install matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97dbb62",
   "metadata": {},
   "source": [
    "**END**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d647d03d",
   "metadata": {},
   "source": [
    "## Working on the Project: PART I - Learning the first game\n",
    "If you run above commands we will see below import modules to be successfully executed in our program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb48b1d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic Python Libraries\n",
    "import os\n",
    "import sys\n",
    "import itertools\n",
    "import random\n",
    "from collections import deque\n",
    "\n",
    "# Gym for loading Atari Environment compatible for Reinforcement Learning\n",
    "import gym\n",
    "from gym.wrappers import Monitor\n",
    "\n",
    "# Basic Data Science Libraries (Useful for Reinforcement Learning)\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image, ImageDraw\n",
    "import keras\n",
    "\n",
    "# Image Processing Libraries\n",
    "import cv2\n",
    "import imageio\n",
    "from skimage.color import rgb2gray\n",
    "from skimage.transform import resize\n",
    "from skimage import img_as_ubyte"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "21c0d9da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Deep Learning: Building Neural Network\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Flatten, Dense, Multiply, Concatenate, LeakyReLU, Lambda, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop, SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18951f3",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Below are the hyperparameters that we are using to tune the learning process of the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b31edc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters space\n",
    "HYPERPARAMS = {\n",
    "    # Google COLAB Setting\n",
    "    \"ON_COLAB\": False,\n",
    "    \n",
    "    # Reinforcement Learning Parameters\n",
    "    \"ENV_NAME\"        : 'BreakoutDeterministic-v4', # Name of environment to be used\n",
    "    # ('SpaceInvaders-v0') # ('Assault-ram-v0')\n",
    "    \"MEM_SIZE\"        : 250000, # Size of replay memory\n",
    "    \"GAMMA\"           : 0.99,   # Gamma (Discount rate) of Markov decision process\n",
    "    \n",
    "    # Exploration vs Exploitation (for Epsilon Decay Policy)\n",
    "    \"EPSILON\"         : 1,     # Start agent at exploration stage=1, or exploitation=0\n",
    "    \"EPSILON_MIN\"     : 0.01,\n",
    "    \"EPSILON_DECAY\"   : 0.9995,\n",
    "    \"TOTAL_FRAMES\"    : 5000000,\n",
    "    \"EPSILON_MAX\"     : 1,\n",
    "    \n",
    "    # Model Training Hyper Parameters\n",
    "    \"LEARNING_RATE\"   : 0.0001,\n",
    "    \"MOMENTUM\"        : 0.001,\n",
    "    \"STACK_SIZE\"      : 4,\n",
    "    \"HIDDEN_NEURONS\"  : 512,  # Number of Neurons in the Deep Neural Network\n",
    "    \"MINIBATCH_SIZE\"  : 32,\n",
    "    \"NUM_EPISODES\"    : 5000, # Number of episodes/gameplay for the agent training\n",
    "    \"RETRAIN\"         : 100,  # Number of times the agent background model trains on its Replay memory before proceeding\n",
    "    \n",
    "    # Model used in Replay Memory for Exploitation (Learning to win) on What has be Explored (or What has been found).\n",
    "    \"TGT_UPDATE_FREQ\" : 1500,\n",
    "    \"NUM_EXPLORE\"     : 1000,\n",
    "    \n",
    "    # For demo purpose\n",
    "    \"VIS_DIR\"               : \"GIFs\", # For GOOGLE COLAB Environment only\n",
    "    \"AUTOSAVE_CHECKPOINT\"   : 100,   # Auto save model after a number of episode = 100\n",
    "    \"SAVED_MODEL_NAME\"      : \"model_dqn_breakout.h5\", # Name of final model, second game \"model_dqn_spaceshoot.h5\"\n",
    "    \"TRANSFER_MODEL_NAME\"   : \"model_dqn_breakout.h5\",\n",
    "    \"TMP_MODEL\"             : 'tmp_model_1.h5',        # TODO: Remove it as its not used yet??\n",
    "    \n",
    "    # Extra Tuning\n",
    "    \"NOOPMAX\"         : 8, # Maximum number of No operation actions taken at the beginning of the game (For using every exploration)\n",
    "    \n",
    "    # Testing\n",
    "    \"NUM_EVAL\"        : 20, # Number of Evals (Test runs)\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d3dbb925",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Lists to store loss and reward value per game!\n",
    "LOSS_HISTORY = []\n",
    "REWARD_HISTORY = []\n",
    "\n",
    "# Number of Frame viewed OR number of times env.step(action) called.\n",
    "FRAME_COUNT = 0\n",
    "\n",
    "model_swap =1\n",
    "if not os.path.exists(\"models\"): \n",
    "    os.mkdir(\"models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8973e18a",
   "metadata": {},
   "source": [
    "### About Reinforcement Learning\n",
    "(To be updated later)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b56db728",
   "metadata": {},
   "source": [
    "### Replay Memory\n",
    "Replay Memory for improving the agent model by making it play (or fitting) over it past experience, stored in it's Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a7cec6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For Boosting Experience\n",
    "class Replay_Memory:\n",
    "    '''\n",
    "        This replay memory clas would act as a buffer in which previous experiences would be stored. \n",
    "        Agent Experience = [ state=current_state, action=current_action, reward, next_state, done]\n",
    "    '''\n",
    "    def __init__(self, MEM_SIZE = 2000): \n",
    "        self.memory = deque(maxlen = MEM_SIZE)\n",
    "        self.max_size = MEM_SIZE\n",
    "    def add(self,  state, action, reward, next_state, done): \n",
    "        self.memory.append(( state, action, reward, next_state, done))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885004f1",
   "metadata": {},
   "source": [
    "### Agent\n",
    "The agent class: contains the following attribute,\n",
    "- Performs Learning using Reinforcement Learning\n",
    "- Saves/Loads the model\n",
    "- Contains Background model and Foreground model\n",
    "- Foreground model plays the game (Exploration)\n",
    "- Background model trains on its Explored Observation (or states/images/frames)\n",
    "- Background model is trained by means of using Replay Memory as mentioned above.\n",
    "- Foreground model is updated after every TGT_UPDATE_FREQ periods on swap count i.e.,(swap_count % TGT_UPDATE_FREQ == 0)\n",
    "- **Special:** Can also do Transfer Learning\n",
    "- Transfer Learning is a ability of making an agent that has learned to play one game to play another game of similar but higher complexity, in a short duration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ee6790fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model/Agent\n",
    "class Agent:\n",
    "    '''This class contains all methods for an agent to function.'''\n",
    "    def __init__(self, env):\n",
    "        print(\"Setting up the agent ...\")\n",
    "        self.state_size = env.observation_space.shape[0]\n",
    "        self.action_size = env.action_space.n\n",
    "        self.memory = Replay_Memory(HYPERPARAMS[\"MEM_SIZE\"])\n",
    "        self.gamma = HYPERPARAMS[\"GAMMA\"]\n",
    "        self.epsilon = HYPERPARAMS[\"EPSILON\"]\n",
    "        self.epsilon_max = HYPERPARAMS[\"EPSILON_MAX\"]\n",
    "        self.epsilon_min = HYPERPARAMS[\"EPSILON_MIN\"]\n",
    "        self.total_frame = HYPERPARAMS[\"TOTAL_FRAMES\"]\n",
    "        self.slope = (self.epsilon_max - self.epsilon_min)/self.total_frame\n",
    "        self.epsilon_decay = HYPERPARAMS[\"EPSILON_DECAY\"]\n",
    "        self.lr = HYPERPARAMS[\"LEARNING_RATE\"]\n",
    "        self.momentum = HYPERPARAMS[\"MOMENTUM\"]\n",
    "        self.dummy_input = np.zeros((1,self.action_size))\n",
    "        self.dummy_batch = np.zeros((HYPERPARAMS[\"MINIBATCH_SIZE\"],self.action_size))\n",
    "        \n",
    "        self.model = self.build_model()\n",
    "        self.target_model = self.get_tgt_model()\n",
    "        \n",
    "        print(\"Agent has been Sucessfully setup ...\")\n",
    "        \n",
    "        \n",
    "    # Function for Agent Model Setup\n",
    "    def lambda_out_shape(self, input_shape):\n",
    "        shape = list(input_shape)\n",
    "        shape[-1] = 1\n",
    "        return tuple(shape)\n",
    "        \n",
    "    \n",
    "    def build_model(self):\n",
    "        '''Model to train the agent'''\n",
    "        return self._build_compatible_model(self.action_size)\n",
    "\n",
    "    def transfer_learning(self, model_pathname, old_actions_size, new_actions_size):\n",
    "        '''\n",
    "            Perform a transfer of knowledege about a game of similar less complex enviroment to this game.\n",
    "            Please supply:\n",
    "            - model_pathname: the location of model file that has already learnt to training on a game.\n",
    "            - old_action_size: the estimation of size for the already learnt game.\n",
    "            - new_action_size: the estimation of size for the current game being played.\n",
    "        '''\n",
    "        return self._build_compatible_model(old_actions_size, True, new_actions_size,model_pathname)\n",
    "\n",
    "    def _build_compatible_model(self, actions_size, is_transfer_learning=False, new_actions_size=0, model_pathname=\"\"):\n",
    "        '''A single method to build model normally or build model after transfer learning from another game'''\n",
    "        prev_actions_size = actions_size\n",
    "\n",
    "        agent_bot = self\n",
    "        input_layer = Input(shape=(84, 84, HYPERPARAMS[\"STACK_SIZE\"]), name=\"image\")     # Sending the stack of 4 resized image of 84*84\n",
    "\n",
    "        # Convolution-Max Pooling parts\n",
    "        conv_layer1 = Conv2D(32, (8, 8), strides=(4, 4), activation='relu', name=\"conv2D_1\")(input_layer)\n",
    "        # May Introduce Max Pooling here ...\n",
    "        conv_layer2 = Conv2D(64, (4, 4), strides=(2, 2), activation='relu', name=\"conv2D_2\")(conv_layer1)\n",
    "        # May Introduce Max Pooling here ...\n",
    "        conv_layer3 = Conv2D(64, (3, 3), strides=(1, 1), activation='relu', name=\"conv2D_3\")(conv_layer2)\n",
    "\n",
    "        # Densely connected Neural Network\n",
    "        flat_feature = Flatten(name=\"flat_1\")(conv_layer3)                               # Input Layer\n",
    "        hidden_feature = Dense(HYPERPARAMS[\"HIDDEN_NEURONS\"], name=\"hidden_layer_1\")(flat_feature)      # Hidden Layer\n",
    "        lrelu_feature = LeakyReLU(name=\"activation_layer\")(hidden_feature)               # using Leaky-Rely activation with alpha=0.3 on Hidden Layer\n",
    "\n",
    "        # Setting up the Output Layer\n",
    "        q_value_prediction = Dense(prev_actions_size, name=\"q_values\")(lrelu_feature)\n",
    "\n",
    "        # Get Single Action and Target Q value\n",
    "        action_one_hot = Input(shape=(prev_actions_size,), name=\"action\")          # Take Current Action to be played\n",
    "        select_q_value_of_action = Multiply()([q_value_prediction,action_one_hot]) # Checking the Q-value of current move/action\n",
    "        target_q_value = Lambda(lambda x:K.max(x, axis=-1, keepdims=True),output_shape=agent_bot.lambda_out_shape)(select_q_value_of_action)\n",
    "\n",
    "        model = Model(inputs=[input_layer,action_one_hot], outputs=[q_value_prediction, target_q_value])\n",
    "\n",
    "        if is_transfer_learning:\n",
    "            # Load model for previous game\n",
    "            model.load_weights(model_pathname)\n",
    "            \n",
    "            # Sibling layers for learning actions\n",
    "            prev_Qlayer = model.get_layer(name=\"q_values\")\n",
    "            prev_Qlayer._name = \"old_action_q_values\"\n",
    "            new_Qlayer = Dense(new_actions_size, name=\"new_action_q_values\")\n",
    "\n",
    "            # Merge the Sibling layers to form one layer to estimate Q-values\n",
    "            q_value_prediction = Concatenate(name=\"q_values\")([prev_Qlayer(lrelu_feature), new_Qlayer(lrelu_feature)])\n",
    "\n",
    "            # Get Single Action and Target Q value\n",
    "            action_one_hot = Input(shape=(prev_actions_size+new_actions_size,), name=\"action\")\n",
    "            select_q_value_of_action = Multiply()([\n",
    "                q_value_prediction,\n",
    "                action_one_hot\n",
    "            ])  \n",
    "            target_q_value = Lambda(lambda x:K.max(x, axis=-1, keepdims=True),output_shape=agent_bot.lambda_out_shape)(select_q_value_of_action)\n",
    "\n",
    "            model = Model(inputs=[input_layer,action_one_hot], outputs=[q_value_prediction, target_q_value])\n",
    "\n",
    "        model.compile(loss=['mse','mse'], loss_weights=[0.0,1.0],optimizer=Adam(agent_bot.lr))\n",
    "        return model\n",
    "        \n",
    "    def get_tgt_model(self):\n",
    "        '''This method would clone the architecture as well as the initial weights of the base model into target model'''\n",
    "        self.target_model = self.build_model()\n",
    "        self.update_target_model()\n",
    "        return self.target_model\n",
    "    \n",
    "    def update_target_model(self): \n",
    "        '''This method would update weights of target model'''\n",
    "        self.target_model.set_weights(self.model.get_weights())\n",
    "    \n",
    "    def load_model(self, pathname): \n",
    "        '''This method would load weights of model'''\n",
    "        # load weights into new model\n",
    "        self.model.load_weights(pathname)\n",
    "        self.target_model = self.get_tgt_model()\n",
    "        print(\"Loaded model from disk\")\n",
    "        \n",
    "    def save_model(self, pathname):\n",
    "        '''Save method would save weights of model model'''\n",
    "        # serialize weights to HDF5\n",
    "        self.model.save_weights(pathname)\n",
    "        self.target_model = self.get_tgt_model()\n",
    "        print(\"Saved model to disk\")\n",
    "        \n",
    "\n",
    "    # Agent Play Prediction function\n",
    "    def next_action(self, state):\n",
    "        '''Get the next action using epsilon greedy policy for deciding whether to exploit or explore'''\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon = self.epsilon_max - self.slope*(FRAME_COUNT)\n",
    "        if (np.random.rand() <= self.epsilon):\n",
    "            return env.action_space.sample()\n",
    "        q_values = self.model.predict([np.expand_dims(state,axis=0),self.dummy_input])[0]\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    # Replay Functions\n",
    "    def store_experience(self, state, action, reward, next_state, done):\n",
    "        '''Store the experience in our replay memory'''\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "\n",
    "    def replay(self, batch_size, model_swap):\n",
    "        '''\n",
    "            Does the back propogation to adjust weights during exploitation action.\n",
    "            - batch_size: total number of random samples that the agent can recollect from memory\n",
    "            - The higher the batch_size more is the time for training process.\n",
    "        '''\n",
    "        # REINFORCEMENT LEARNING\n",
    "        print(\"Game Play Paused! Model is training on it's past Memory\")\n",
    "        # First we set all input to NOOP or no move for every observations(stack of frames or images)\n",
    "        # Dummy_Inputs_batch.shape = [(MINBATCH_SIZE = 32 images), (action_size = 4 moves for breakout)]\n",
    "        dummy_batch = np.zeros((batch_size,self.action_size)) \n",
    "        \n",
    "        # Experience batch set\n",
    "        state_batch      = []\n",
    "        action_batch     = []\n",
    "        reward_batch     = []\n",
    "        next_state_batch = []\n",
    "        terminal_batch   = []  # recording Is_done?\n",
    "        \n",
    "        # Actual Move that should have played (This is also an Assumption)\n",
    "        y_batch = []\n",
    "\n",
    "        # Sample random minibatch of transition from replay memory\n",
    "        minibatch = random.sample(list(self.memory.memory), batch_size)\n",
    "        # For every experience thats in our Replay Memory\n",
    "        for data in minibatch: # We organize the data\n",
    "            state_batch.append(data[0])\n",
    "            action_batch.append(data[1])\n",
    "            reward_batch.append(data[2])\n",
    "            next_state_batch.append(data[3])\n",
    "            terminal_batch.append(data[4])\n",
    "        \n",
    "        # Convert the is_done to a numpy array\n",
    "        terminal_batch = np.array(terminal_batch)\n",
    "\n",
    "         \n",
    "        for i in np.arange(HYPERPARAMS[\"RETRAIN\"]):\n",
    "            # Get what agent is assuming with the trained model till now. Supplying NOOP/NoAction input for every move... \n",
    "            # Model is predicting the Q-Value or we can all it as Future reward from current move (or action) made\n",
    "            target_q_values_batch = self.target_model.predict([np.float32(np.array(next_state_batch)),self.dummy_batch])[0]\n",
    "            # What model should assume (The Assumption is to predict its own output without any loss) Outrageous!!\n",
    "            y_batch = reward_batch + (1 - terminal_batch) * self.gamma * np.max(target_q_values_batch, axis=-1)\n",
    "            # (1 - terminal_batch) above is to indicate the game is done(0, return only reward) or not(1, return reward with discounted sum)\n",
    "            # y_batch is also called Future Reward or the reward model is expecting to get in the future.\n",
    "\n",
    "\n",
    "            a_one_hot = np.zeros((batch_size,self.action_size))\n",
    "            for index,action in enumerate(action_batch):\n",
    "                a_one_hot[index,action] = 1.0            # Get the Action the player performed previously\n",
    "\n",
    "            # START TRAINING PROCESS\n",
    "            # Get the loss between NoAction and the Expected Action that model suplies\n",
    "            loss = self.model.train_on_batch([np.float32(np.array(state_batch)),a_one_hot],[self.dummy_batch,y_batch])\n",
    "\n",
    "            if i == HYPERPARAMS[\"RETRAIN\"]-1: # Append loss to it's history, only on the last re-train loop\n",
    "                LOSS_HISTORY.append(loss[1])\n",
    "        \n",
    "            # END TRAINING PROCESS\n",
    "        \n",
    "        #At target network's update frequency, update the target network\n",
    "        if(model_swap % HYPERPARAMS[\"TGT_UPDATE_FREQ\"] == 0):\n",
    "            self.update_target_model()  # Swap Models\n",
    "            print(\"Target model swapped successfully with Original model!\")\n",
    "            \n",
    "        return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7e8ee6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "\n",
    "def preprocess(image):\n",
    "    '''This method downsamples and resizes the images to 84*84 and converts it to grayscale for CNN compatibility and processing efficiency'''\n",
    "    # Downsample(image) & resize image into square for CNN compatibility\n",
    "    resized_image = preprocess_rgb(image)\n",
    "    grayscale_image = rgb2gray(resized_image)\n",
    "    return grayscale_image\n",
    "\n",
    "def preprocess_rgb(image):\n",
    "    '''This method downsamples and resizes the images to 84*84 and converts it to grayscale for CNN compatibility and processing efficiency'''\n",
    "    # Downsample(image) & resize image into square for CNN compatibility\n",
    "    resized_image = cv2.resize(image[::2, ::2], (84, 84), interpolation = cv2.INTER_AREA)\n",
    "    return resized_image\n",
    "\n",
    "def generate_gif(frame_no, frames, reward, path, e):\n",
    "    '''Utility method to generate gif from frames'''\n",
    "    for idx, frame_idx in enumerate(frames): \n",
    "        frames[idx] = resize(frame_idx, (420, 320, 3), preserve_range=True, order=0).astype(np.uint8)\n",
    "        \n",
    "    imageio.mimsave(f'{path}{\"episode_{0}_frame_{1}_reward_{2}.gif\".format(e, frame_no, reward)}', frames, duration=1/100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9af08",
   "metadata": {},
   "source": [
    "### Starting the Game Environment\n",
    "As seen previously we have installed the environment for running any game within gym emulator setup. Now is the time to get things working in action, for the main aim of this project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1e955804",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rahul\\miniconda3\\envs\\py36\\lib\\site-packages\\ale_py\\roms\\utils.py:90: DeprecationWarning: SelectableGroups dict interface is deprecated. Use select.\n",
      "  for external in metadata.entry_points().get(self.group, []):\n",
      "C:\\Users\\rahul\\miniconda3\\envs\\py36\\lib\\site-packages\\ale_py\\roms\\__init__.py:94: DeprecationWarning: Automatic importing of atari-py roms won't be supported in future releases of ale-py. Please migrate over to using `ale-import-roms` OR an ALE-supported ROM package. To make this warning disappear you can run `ale-import-roms --import-from-pkg atari_py.atari_roms`.For more information see: https://github.com/mgbellemare/Arcade-Learning-Environment#rom-management\n",
      "  _RESOLVED_ROMS = _resolve_roms()\n"
     ]
    }
   ],
   "source": [
    "from ale_py import ALEInterface\n",
    "from ale_py.roms import Breakout, SpaceInvaders,Tetris\n",
    "ale = ALEInterface()\n",
    "ale.loadROM(Breakout) #ale.loadROM(Breakout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b7c716e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(HYPERPARAMS[\"ENV_NAME\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b0dff4f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENVIRONMENT BreakoutDeterministic-v4:\n",
      "This environment requires 4 actions.\n",
      "The actions are ['NOOP', 'FIRE', 'RIGHT', 'LEFT'].\n"
     ]
    }
   ],
   "source": [
    "print(f'ENVIRONMENT {HYPERPARAMS[\"ENV_NAME\"]}:')\n",
    "print(f'This environment requires {env.action_space.n} actions.')\n",
    "print(f'The actions are {env.unwrapped.get_action_meanings()}.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9c62ec12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up the agent ...\n",
      "Agent has been Sucessfully setup ...\n"
     ]
    }
   ],
   "source": [
    "agent = Agent(env)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a74f5574",
   "metadata": {},
   "source": [
    "### A short demo: Of how the model predicts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1259284a",
   "metadata": {},
   "source": [
    "For the first step we are delivering the **STACK_SIZE=4** number of images at a time in our agent model. Along with this is the current action being played. The action signifies the last move that was played, i.e. the move played by in the last image of the 4 image stack/sequence.\n",
    "\n",
    "You may wonder why we are considering the **STACK_SIZE**. Firstly, DeepMind choose to use the past 4 frames. Why?\n",
    "1. frame doesnot describe the movement of player or the enemies or any items. (Relative motion of any object)\n",
    "2. frames bare minimum requirement to learn about the speed of objects. (We capture the relative position on object between 2 frames)\n",
    "3. frames is necessary to infer acceleration. Why? \n",
    "   - Every frame we are received with, provides the derivative of position w.r.t time.\n",
    "4. and so on..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ee6fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.reset()\n",
    "image1, image2, image3, image4 = preprocess(env.step(1)[0]), preprocess(env.step(2)[0]), preprocess(env.step(0)[0]), preprocess(env.step(0)[0])\n",
    "cv2.imshow(\"image1\", image1)\n",
    "cv2.imshow(\"image2\", image2)\n",
    "cv2.imshow(\"image3\", image3)\n",
    "cv2.imshow(\"image4\", image4)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25db0382",
   "metadata": {},
   "source": [
    "4 windows pop up showcasing how the images look like. \n",
    "\n",
    "**Warning: Hit Space or any button to Resume. Else, your IPython Kernel may die/crash.**\n",
    "\n",
    "By executing below you are letting make it's first prediction. As said before the model takes STACK_SIZE=4 images and the current action (in one hot encoded format)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32018888",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.model.predict([\n",
    "    np.expand_dims(\n",
    "        np.stack([\n",
    "            image1, image2, image3, image4\n",
    "        ], axis=2), \n",
    "        axis=0\n",
    "    ), np.array([[0,1,0,0]],dtype='float')\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb7eca92",
   "metadata": {},
   "source": [
    "### Q-Learning\n",
    "The first four floats, shown above, are the Q-values for each action move that can be played in the game. Also, the Q-values are for the action for the current state. The maximum of these Q-values is the **target output** or y, which we use in our **REPLAY MEMORY OF AGENT** as the expected output.\n",
    "\n",
    "Process of Reinforcement Learning using Q-Learning\n",
    "- Get Q-values from Neural Network\n",
    "- use Target_Qvalue_action_i = r+max(Q-values), or \n",
    "  - Future Reward for the action_i on state_s is ( current_reward + max(next_predicted_future_reward) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ab9804ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slow_start(env,image_stack=[], rgb_stack=[],NOOPMAX=10):\n",
    "    idle_times = random.randint(4, NOOPMAX)\n",
    "    \n",
    "    #print(f'Agent is Staying Idle for {idle_times} times. Agent is thinking about what move to make...')\n",
    "    for idle_time in range(idle_times):\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]==False: env.render() # NOTE: Comment this in Google Colab\n",
    "        state, reward, done, info = env.step(0) # Zero means: NOOP or No Operation\n",
    "        processed_frame = preprocess(state)\n",
    "        image_stack.append(processed_frame)\n",
    "        rgb_stack.append(preprocess_rgb(state))\n",
    "        \n",
    "    return image_stack, rgb_stack"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89cb94bd",
   "metadata": {},
   "source": [
    "### Model Checkpointing\n",
    "During the training process we found that it is really inefficient to produce the complete training model of 30hrs in a single run. It's better to work checkpoint of 5-10 hrs and save the progress in middle. For this we have devised the load_model and save_model functionality within our agent class.\n",
    "\n",
    "Use below function to check the trained model performance after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23ab2138",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "Unable to open file (unable to open file: name = './models/model_dqn_breakout.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-b2cd0f35c688>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'./models/model_dqn_breakout.h5'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-6-4a6f75a4c3e0>\u001b[0m in \u001b[0;36mload_model\u001b[1;34m(self, pathname)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \u001b[1;34m'''This method would load weights of model'''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    114\u001b[0m         \u001b[1;31m# load weights into new model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpathname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_tgt_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Loaded model from disk\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m     \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[1;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, mode, driver, libver, userblock_size, swmr, rdcc_nslots, rdcc_nbytes, rdcc_w0, track_order, **kwds)\u001b[0m\n\u001b[0;32m    406\u001b[0m                 fid = make_fid(name, mode, userblock_size,\n\u001b[0;32m    407\u001b[0m                                \u001b[0mfapl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfcpl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_fcpl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrack_order\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 408\u001b[1;33m                                swmr=swmr)\n\u001b[0m\u001b[0;32m    409\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    410\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlibver\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\h5py\\_hl\\files.py\u001b[0m in \u001b[0;36mmake_fid\u001b[1;34m(name, mode, userblock_size, fapl, fcpl, swmr)\u001b[0m\n\u001b[0;32m    171\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mswmr\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mswmr_support\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m             \u001b[0mflags\u001b[0m \u001b[1;33m|=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_SWMR_READ\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 173\u001b[1;33m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mflags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    174\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'r+'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mfid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mh5f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mACC_RDWR\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfapl\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfapl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mh5py\\h5f.pyx\u001b[0m in \u001b[0;36mh5py.h5f.open\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mOSError\u001b[0m: Unable to open file (unable to open file: name = './models/model_dqn_breakout.h5', errno = 2, error message = 'No such file or directory', flags = 0, o_flags = 0)"
     ]
    }
   ],
   "source": [
    "agent.load_model('./models/model_dqn_breakout.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c37470",
   "metadata": {},
   "source": [
    "**OR** run below code block to run initial exploration stage for our agent to get an understanding of the game. This is not exactly understanding, but a way for us to fill the replay memory with images of the game being played with random actions.\n",
    "\n",
    "### Exploration (Run only if you are avoiding Model Checkpointing step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "8170cafb",
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPLORE = 1\n",
    "TRAIN   = 2\n",
    "TEST    = 3\n",
    "def gameplay(PLAY_TYPE=TEST,MAX_EPISODE_PLAYTIME=1000000):\n",
    "    global FRAME_COUNT\n",
    "    #print(f'Agent is starting a new game: {e} games played.')\n",
    "    \n",
    "    # Reset Game\n",
    "    state = env.reset()\n",
    "    times_rewarded = times_penalized = 0\n",
    "    last_lives = 5\n",
    "    terminal_life_lost = False # False if last_lives==0 else True\n",
    "    \n",
    "    #print('Agent has made the start move.')\n",
    "    # Start the game by 'FIRE' action, incase if it doesnot start the game without it\n",
    "    state, _, _, _ = env.step(1) \n",
    "    \n",
    "    # Fill agent's memory with random times of no operation played\n",
    "    image_stack,rgb_stack = slow_start(env=env, NOOPMAX=HYPERPARAMS[\"NOOPMAX\"])\n",
    "    \n",
    "    \n",
    "    #print(f'Agent is now playing the game...')\n",
    "    i, state = 0, np.stack(image_stack[-4:], axis = 2)\n",
    "    while i < MAX_EPISODE_PLAYTIME:\n",
    "        agent.epsilon = 1  # Agent is Exploring the game by default\n",
    "        \n",
    "        # If agent has lost a life then start the game with 'FIRE' again.\n",
    "        if(terminal_life_lost == True):\n",
    "            state, _, _, _ = env.step(1) # 'FIRE' to start the game\n",
    "            slow_start(env, image_stack, rgb_stack, HYPERPARAMS[\"NOOPMAX\"])\n",
    "            state = np.stack(image_stack[-4:], axis = 2)\n",
    "\n",
    "        FRAME_COUNT = FRAME_COUNT + 1\n",
    "        action = env.action_space.sample() if PLAY_TYPE==EXPLORE else (agent.next_action(state) if PLAY_TYPE==TRAIN else np.argmax(agent.model.predict([np.expand_dims(state,axis=0),agent.dummy_input])[0]))\n",
    "\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]==False: env.render() # NOTE: Comment this in Google Colab\n",
    "        # Agent Makes random moves here...\n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        rgb_stack.append(preprocess_rgb(next_state))\n",
    "        \n",
    "        # Agent updates it's game status here...\n",
    "        terminal_life_lost = True if info['ale.lives'] < last_lives else done\n",
    "        last_lives = info['ale.lives']\n",
    "        \n",
    "            \n",
    "        if reward > 0:      times_rewarded = times_rewarded + 1\n",
    "        elif reward < 0: times_penalized = times_penalized + 1\n",
    "        elif terminal_life_lost: times_penalized = times_penalized + 1\n",
    "        # Making the starting experience of rewards more fruitful. For our replay memory...\n",
    "        reward = 10 if reward > 0 else (-30 if reward < 0 else reward)\n",
    "        reward = -30 if terminal_life_lost else reward\n",
    "        \n",
    "        # Store the stack of images for new a experience\n",
    "        processed_frame = preprocess(next_state)\n",
    "        image_stack = image_stack[-3:]\n",
    "        image_stack.append(processed_frame)\n",
    "        \n",
    "        next_state = np.stack(image_stack[-4:], axis = 2)\n",
    "        if(len(image_stack) != 4): print(\"Something's not right!! The stack size is less than expected.\")\n",
    "            \n",
    "        #Store experience in replay mem\n",
    "        if(PLAY_TYPE==EXPLORE or PLAY_TYPE==TRAIN): \n",
    "            agent.store_experience(state, action, reward, next_state, terminal_life_lost)\n",
    "        state = next_state\n",
    "        \n",
    "        if done: break\n",
    "        i+=1\n",
    "        \n",
    "    REWARD_HISTORY.append(times_rewarded)\n",
    "    return image_stack, times_rewarded, times_penalized, rgb_stack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b4fcea1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished exploring for 0 episodes\n",
      "Total Times Rewarded: 0, Total Times Penalized: 5\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-1bb91cf8788f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mtotal_times_rewarded\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtotal_times_penalized\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"NUM_EXPLORE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mimage_stack\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes_rewarded\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimes_penalized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrgb_stack\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgameplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mEXPLORE\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAX_EPISODE_PLAYTIME\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1000\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mtotal_times_rewarded\u001b[0m  \u001b[1;33m=\u001b[0m \u001b[0mtotal_times_rewarded\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mtimes_rewarded\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mtotal_times_penalized\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtotal_times_penalized\u001b[0m\u001b[1;33m+\u001b[0m \u001b[0mtimes_penalized\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-15-87a60be564db>\u001b[0m in \u001b[0;36mgameplay\u001b[1;34m(PLAY_TYPE, MAX_EPISODE_PLAYTIME)\u001b[0m\n\u001b[0;32m     34\u001b[0m         \u001b[0maction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msample\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mEXPLORE\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_action\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mPLAY_TYPE\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mTRAIN\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpand_dims\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdummy_input\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 36\u001b[1;33m         \u001b[1;32mif\u001b[0m \u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"ON_COLAB\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m==\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# NOTE: Comment this in Google Colab\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     37\u001b[0m         \u001b[1;31m# Agent Makes random moves here...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[0mnext_state\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0maction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\gym\\core.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode, **kwargs)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    253\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"human\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 254\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0menv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    255\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    256\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\gym\\envs\\atari\\atari_env.py\u001b[0m in \u001b[0;36mrender\u001b[1;34m(self, mode)\u001b[0m\n\u001b[0;32m    168\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrendering\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mSimpleImageViewer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\gym\\envs\\classic_control\\rendering.py\u001b[0m in \u001b[0;36mimshow\u001b[1;34m(self, arr)\u001b[0m\n\u001b[0;32m    439\u001b[0m             \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"RGB\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtobytes\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpitch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    440\u001b[0m         )\n\u001b[1;32m--> 441\u001b[1;33m         \u001b[0mtexture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_texture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    442\u001b[0m         \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mglTexParameteri\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGL_TEXTURE_2D\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGL_TEXTURE_MAG_FILTER\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgl\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGL_NEAREST\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    443\u001b[0m         \u001b[0mtexture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\pyglet\\image\\__init__.py\u001b[0m in \u001b[0;36mget_texture\u001b[1;34m(self, rectangle, force_rectangle)\u001b[0m\n\u001b[0;32m    833\u001b[0m         if (not self._current_texture or\n\u001b[0;32m    834\u001b[0m                 (not self._current_texture._is_rectangle and force_rectangle)):\n\u001b[1;32m--> 835\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_texture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_texture\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTexture\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrectangle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mforce_rectangle\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    836\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_current_texture\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\pyglet\\image\\__init__.py\u001b[0m in \u001b[0;36mcreate_texture\u001b[1;34m(self, cls, rectangle, force_rectangle)\u001b[0m\n\u001b[0;32m    820\u001b[0m         \u001b[0minternalformat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_internalformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    821\u001b[0m         texture = cls.create(self.width, self.height, internalformat,\n\u001b[1;32m--> 822\u001b[1;33m                              rectangle, force_rectangle)\n\u001b[0m\u001b[0;32m    823\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_y\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    824\u001b[0m             \u001b[0mtexture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_x\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0manchor_x\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\pyglet\\image\\__init__.py\u001b[0m in \u001b[0;36mcreate\u001b[1;34m(cls, width, height, internalformat, rectangle, force_rectangle, min_filter, mag_filter)\u001b[0m\n\u001b[0;32m   1474\u001b[0m                      \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1475\u001b[0m                      \u001b[0mGL_RGBA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mGL_UNSIGNED_BYTE\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1476\u001b[1;33m                      blank)\n\u001b[0m\u001b[0;32m   1477\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1478\u001b[0m         \u001b[0mtexture\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcls\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtexture_width\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtexture_height\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\pyglet\\gl\\lib.py\u001b[0m in \u001b[0;36merrcheck\u001b[1;34m(result, func, arguments)\u001b[0m\n\u001b[0;32m     85\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 87\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0merrcheck\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0marguments\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     88\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0m_debug_gl_trace\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "    THIS IS DONE TO POPULATE REPLAY MEMORY WITH COMPLETE EXPLORATION TO INITIALIZE THE MEMORY \n",
    "'''\n",
    "total_times_rewarded=total_times_penalized=0\n",
    "for e in range(HYPERPARAMS[\"NUM_EXPLORE\"]):\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=EXPLORE, MAX_EPISODE_PLAYTIME=1000)\n",
    "    total_times_rewarded  = total_times_rewarded + times_rewarded\n",
    "    total_times_penalized = total_times_penalized+ times_penalized\n",
    "    if(e % 100 == 0): \n",
    "        print(\"Finished exploring for {} episodes\".format(e))\n",
    "        print(\"Total Times Rewarded: {}, Total Times Penalized: {}\".format(total_times_rewarded, total_times_penalized))\n",
    "\n",
    "print(\"EXPLORATION STEP COMPLETED\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22f1bef6",
   "metadata": {},
   "source": [
    "### TRAINING STAGE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "159a3e5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n",
      "Agent is ready for gameplay...\n",
      "Game Play Paused! Model is training on it's past Memory\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-2003fcfb04dc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[1;31m# BACKPROP INITIATED AT THE END OF EVERY EPISODE AND NOT AT TGT_FREQ\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 24\u001b[1;33m     \u001b[0magent\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreplay\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mHYPERPARAMS\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"MINIBATCH_SIZE\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel_swap\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     25\u001b[0m     \u001b[0mmodel_swap\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel_swap\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-4a6f75a4c3e0>\u001b[0m in \u001b[0;36mreplay\u001b[1;34m(self, batch_size, model_swap)\u001b[0m\n\u001b[0;32m    179\u001b[0m             \u001b[1;31m# Get what agent is assuming with the trained model till now. Supplying NOOP/NoAction input for every move...\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;31m# Model is predicting the Q-Value or we can all it as Future reward from current move (or action) made\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 181\u001b[1;33m             \u001b[0mtarget_q_values_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtarget_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext_state_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdummy_batch\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    182\u001b[0m             \u001b[1;31m# What model should assume (The Assumption is to predict its own output without any loss) Outrageous!!\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0my_batch\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreward_batch\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mterminal_batch\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtarget_q_values_batch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\keras\\utils\\traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 64\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     65\u001b[0m     \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[1;31m# pylint: disable=broad-except\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mpredict\u001b[1;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[0;32m   1766\u001b[0m           \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1767\u001b[0m           \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1768\u001b[1;33m           steps_per_execution=self._steps_per_execution)\n\u001b[0m\u001b[0;32m   1769\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1770\u001b[0m       \u001b[1;31m# Container that configures and calls `tf.keras.Callback`s.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36mget_data_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1401\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"model\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"_cluster_coordinator\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1402\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_ClusterCoordinatorDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1403\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mDataHandler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1405\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weight, batch_size, steps_per_epoch, initial_epoch, epochs, shuffle, class_weight, max_queue_size, workers, use_multiprocessing, model, steps_per_execution, distribute)\u001b[0m\n\u001b[0;32m   1163\u001b[0m         \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1164\u001b[0m         \u001b[0mdistribution_strategy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1165\u001b[1;33m         model=model)\n\u001b[0m\u001b[0;32m   1166\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1167\u001b[0m     \u001b[0mstrategy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdistribute\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_strategy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\keras\\engine\\data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[0;32m    289\u001b[0m     \u001b[1;31m# trigger the next permutation. On the other hand, too many simultaneous\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;31m# shuffles can contend on a hardware level and degrade all performance.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 291\u001b[1;33m     \u001b[0mindices_dataset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmap\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpermutation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprefetch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    292\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    293\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mslice_batch_indices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[1;34m(self, map_func, num_parallel_calls, deterministic, name)\u001b[0m\n\u001b[0;32m   2002\u001b[0m         warnings.warn(\"The `deterministic` argument has no effect unless the \"\n\u001b[0;32m   2003\u001b[0m                       \"`num_parallel_calls` argument is specified.\")\n\u001b[1;32m-> 2004\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mMapDataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpreserve_cardinality\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2005\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2006\u001b[0m       return ParallelMapDataset(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, input_dataset, map_func, use_inter_op_parallelism, preserve_cardinality, use_legacy_function, name)\u001b[0m\n\u001b[0;32m   5457\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5458\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5459\u001b[1;33m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[0;32m   5460\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_metadata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdataset_metadata_pb2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMetadata\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5461\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\data\\ops\\dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[0;32m   4531\u001b[0m         \u001b[0mfn_factory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtrace_tf_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdefun_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4532\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4533\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfn_factory\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4534\u001b[0m     \u001b[1;31m# There is no graph to add in eager mode.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4535\u001b[0m     \u001b[0madd_to_graph\u001b[0m \u001b[1;33m&=\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcontext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mget_concrete_function\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3243\u001b[0m     \"\"\"\n\u001b[0;32m   3244\u001b[0m     graph_function = self._get_concrete_function_garbage_collected(\n\u001b[1;32m-> 3245\u001b[1;33m         *args, **kwargs)\n\u001b[0m\u001b[0;32m   3246\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_garbage_collector\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrelease\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3247\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_get_concrete_function_garbage_collected\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3208\u001b[0m       \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3209\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3210\u001b[1;33m       \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3211\u001b[0m       \u001b[0mseen_names\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3212\u001b[0m       captured = object_identity.ObjectIdentitySet(\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   3555\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3556\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3557\u001b[1;33m           \u001b[0mgraph_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3558\u001b[0m           \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3559\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[1;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[0;32m   3407\u001b[0m         \u001b[1;31m# places (like Keras) where the FuncGraph lives longer than the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3408\u001b[0m         \u001b[1;31m# ConcreteFunction.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3409\u001b[1;33m         shared_func_graph=False)\n\u001b[0m\u001b[0;32m   3410\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3411\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, shared_func_graph, function_spec)\u001b[0m\n\u001b[0;32m   1592\u001b[0m     \u001b[1;31m# FuncGraph directly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m     self._delayed_rewrite_functions = _DelayedRewriteGradientFunctions(\n\u001b[1;32m-> 1594\u001b[1;33m         func_graph, self._attrs, self._garbage_collector)\n\u001b[0m\u001b[0;32m   1595\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_first_order_tape_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1596\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_higher_order_tape_functions\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, func_graph, attrs, func_graph_deleter)\u001b[0m\n\u001b[0;32m    683\u001b[0m     self._inference_function = _EagerDefinedFunction(\n\u001b[0;32m    684\u001b[0m         \u001b[0m_inference_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_func_graph\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         self._func_graph.inputs, self._func_graph.outputs, attrs)\n\u001b[0m\u001b[0;32m    686\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_attrs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_gradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\miniconda3\\envs\\py36\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, name, graph, inputs, outputs, attrs)\u001b[0m\n\u001b[0;32m    485\u001b[0m         \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# control_output_names\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    486\u001b[0m         \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 487\u001b[1;33m         compat.as_str(\"\"))\n\u001b[0m\u001b[0;32m    488\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    489\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattr_value\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "TRAIN DQN ON THE GAME\n",
    "'''\n",
    "rew_list = []\n",
    "e=0\n",
    "for e in range(1, HYPERPARAMS[\"NUM_EPISODES\"]+1):\n",
    "    print(\"Agent is ready for gameplay...\")\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=TRAIN, MAX_EPISODE_PLAYTIME=1000)\n",
    "            \n",
    "    rew_list.append(times_rewarded)\n",
    "            \n",
    "    if(e % HYPERPARAMS[\"AUTOSAVE_CHECKPOINT\"] == 0):\n",
    "        print(\"Finished episode \", e , \"/\", HYPERPARAMS[\"NUM_EPISODES\"], \" Total reward = \", sum(rew_list)/len(rew_list), \" eps = \", agent.epsilon)\n",
    "        if HYPERPARAMS[\"ON_COLAB\"]:\n",
    "            if not os.path.exists(HYPERPARAMS[\"VIS_DIR\"]):\n",
    "                os.mkdir(HYPERPARAMS[\"VIS_DIR\"])\n",
    "            generate_gif(len(rgb_stack), rgb_stack, sum(rew_list)/(len(rew_list)), HYPERPARAMS[\"VIS_DIR\"] + \"/\", e)\n",
    "        \n",
    "        rew_list = []\n",
    "        print(f\"Saving Model Checkpoint at episode {e}...\")\n",
    "        agent.save_model(\"models/tmp_model_\" + str(e) + \".h5\")\n",
    "\n",
    "    # BACKPROP INITIATED AT THE END OF EVERY EPISODE AND NOT AT TGT_FREQ\n",
    "    agent.replay(HYPERPARAMS[\"MINIBATCH_SIZE\"], model_swap)\n",
    "    model_swap = model_swap + 1\n",
    "    \n",
    "print(f\"Agent is now prepared now, after training for {e} episodes.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031e1414",
   "metadata": {},
   "source": [
    "#### Saving the final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b14517",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.save_model(agent.model, \"models/\" + HYPERPARAMS[\"MODEL_NAME\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4512222e",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "aa84b109",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished episode  0 / 20  Total reward =  10\n",
      "Finished episode  1 / 20  Total reward =  30\n",
      "Finished episode  2 / 20  Total reward =  10\n",
      "Finished episode  3 / 20  Total reward =  0\n",
      "Finished episode  4 / 20  Total reward =  40\n",
      "Finished episode  5 / 20  Total reward =  10\n",
      "Finished episode  6 / 20  Total reward =  0\n",
      "Finished episode  7 / 20  Total reward =  30\n",
      "Finished episode  8 / 20  Total reward =  10\n",
      "Finished episode  9 / 20  Total reward =  10\n",
      "Finished episode  10 / 20  Total reward =  20\n",
      "Finished episode  11 / 20  Total reward =  30\n",
      "Finished episode  12 / 20  Total reward =  10\n",
      "Finished episode  13 / 20  Total reward =  20\n",
      "Finished episode  14 / 20  Total reward =  10\n",
      "Finished episode  15 / 20  Total reward =  30\n",
      "Finished episode  16 / 20  Total reward =  0\n",
      "Finished episode  17 / 20  Total reward =  10\n",
      "Finished episode  18 / 20  Total reward =  10\n",
      "Finished episode  19 / 20  Total reward =  20\n",
      "Testing Complete\n"
     ]
    }
   ],
   "source": [
    "rew_list = []\n",
    "total_times_rewarded=total_times_penalized=0\n",
    "for e in range(HYPERPARAMS[\"NUM_EVAL\"]):\n",
    "    image_stack, times_rewarded, times_penalized, rgb_stack = gameplay(PLAY_TYPE=EXPLORE, MAX_EPISODE_PLAYTIME=1000)\n",
    "    \n",
    "    total_times_rewarded  = total_times_rewarded + times_rewarded\n",
    "    total_times_penalized = total_times_penalized+ times_penalized\n",
    "    rew_list.append(times_rewarded)\n",
    "    \n",
    "    print(\"Finished episode \", e , \"/\", HYPERPARAMS[\"NUM_EVAL\"], \" Total reward = \", times_rewarded*(10))\n",
    "    if HYPERPARAMS[\"ON_COLAB\"]:\n",
    "        if not os.path.exists(\"test\"):\n",
    "            os.mkdir(\"test\")\n",
    "        generate_gif(len(rgb_stack), rgb_stack, total_times_rewarded, \"test/\", e)\n",
    "\n",
    "print(\"Testing Complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3962cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()\n",
    "exit(1) # Close Gym Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61493c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
